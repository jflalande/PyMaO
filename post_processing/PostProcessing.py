import os
import time
import logging  # Log info output
import datetime
import argparse  # Program argument parser
import json
import collections  # imported for OrderedDict

from os import listdir
from os.path import isfile, join

import BooleanParser

import sys

from collections import Counter # For getting top or botton N
from openpyxl import Workbook   # For outputing results in xlsx (Excel format)
import datetime as dt

import numpy as np
# import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.ticker as ticker
# import guess_distribution as gd
import scipy.stats as st

from math import floor, log as loga
"""
Usage:
    python3 <this_file> <json_config>

where:
    json_config: The file that holds the definitions of the rows and columns to be processed

Example of config file:

see `post_processing/examples/drebin.json`

    {
        "rows":{
            "Dataset 1":"/path/to/dataset/1",  ----> Directories that contain JSON
            "Dataset 2":"/path/to/dataset/2",        files generated by the orchestrator
            "Dataset 3":"/path/to/dataset/3",
        },
        "columns":{
            "Column 1":[
                "<boolean expression with JSONPath>",
                null        ---------------------------> No special porcentage is done
            ],
            "Column 2":[
                "<boolean expression with JSONPath>",
                null
            ],
            "Column 3":[
                "<boolean expression with JSONPath>",
                "Column 2"               ------------> Porcentage in relation to Column 2,
            ]                                          which MUST be declared before
        },
        "histograms":{
            "Data 1":[
    			"<JSONPath expression>",
    			"<type>"             ---------------> This can be date, int, float, string
    		],
    		"Data 2":[
    			"<JSONPath expression>",
    			"<type>"
    		]
        },
        "output_dir":"/path/to/your/output/dir"
    }


The JSONPath implementation is from (https://github.com/h2non/jsonpath-ng)

The boolean expression parser implementation is from (https://gist.github.com/leehsueh/1290686)

It follows this grammar:

    Expression --> Terminal (>,<,>=,<=,==) Terminal
    Terminal --> Number or String or Variable

To compare a variable to a string, they must be enclosed in <""> in the JSON file. For example:

    $..Unzip.status == \"done\"

This expression compares if the variable `$..Unzip.status` is equal to the string `done`.

For our purposes, all the JSONPath expressions are variables.

### TODO:
* [ ]  Expand expression for multiple requests
"""

# Name without extension
OUTPUT_FILENAME = "res"
# Adds a very verbose level of logs
DEBUG_LEVELV_NUM = 9
logging.addLevelName(DEBUG_LEVELV_NUM, "DEBUGV")


def debugv(self, message, *args, **kws):
    # Yes, logger takes its '*args' as 'args'.
    if self.isEnabledFor(DEBUG_LEVELV_NUM):
        self._log(DEBUG_LEVELV_NUM, message, args, **kws)


logging.Logger.debugv = debugv
log = logging.getLogger("post-processing")


# Tries to apply colors to logs
def applyColorsToLogs():
    try:
        import coloredlogs

        style = coloredlogs.DEFAULT_LEVEL_STYLES
        style['debugv'] = {'color': 'magenta'}
        coloredlogs.install(
            show_hostname=False, show_name=True,
            logger=log,
            level=DEBUG_LEVELV_NUM,
            fmt='%(asctime)s [%(levelname)8s] %(message)s'
            # Default format:
            # fmt='%(asctime)s %(hostname)s %(name)s[%(process)d] %(levelname)s
            #  %(message)s'
        )
    except ImportError:
        log.error("Can't import coloredlogs, logs may not appear correctly.")


def logSetup(level):
    # if -vv option as program argument
    if level == 2:
        log.setLevel(DEBUG_LEVELV_NUM)
        log.info("Debug is Very Verbose.")
    # if -v option as program argument
    elif level == 1:
        log.setLevel(logging.DEBUG)
        log.info("Debug is Verbose.")
    # if no (-v) option
    elif level == 0:
        log.setLevel(logging.INFO)
        log.info("Debug is Normal.")
    else:
        # else
        log.setLevel(logging.INFO)
        log.warning("Logging level \"{}\" not defined, setting \"normal\" instead"
                    .format(level))


applyColorsToLogs()

################################################################################
#                                                                              #
#                             Function definitions                             #
#                                                                              #
################################################################################


def epoch_to_date(epoch):
    return datetime.datetime.fromtimestamp(epoch).strftime('%Y-%m-%d %H:%M:%S')


os.stat_float_times(False)


def output_histograms(datasets, histograms_def, output_dir):

    # Pretty colors for the delight of the eye
    plt.style.use('seaborn-deep')

    # joint_histogram = {}
    # for histogram in histograms_def:
    #     joint_histogram[name] = {'type':histograms_def[1], 'data'=[]}

    for histogram_name in histograms_def:
        log.info("Processing histogram " + histogram_name)
        # joint_histogram = {}
        joint_histogram = {'type': histograms_def[histogram_name][1], 'data': {}}

        # rows = []

        for row_name in datasets:

            # log.debug("Assigning row " + row_name)
            log.debugv("Assigning row " + row_name)
            row = datasets[row_name]

            # Print individual histograms
            # Verify if there are histograms to output
            # if len(row.histogram_collection.keys()) != 0:
            #     for histogram_name in row.histogram_collection:
            #     def output_histograms(data,output_dir):

            data = row.histogram_collection[histogram_name].data

            # backup = []
            # original_data = data
            # for value in data:
            #     if value < (37 * 10**6):
            #         backup.append(value)
            # data = backup

            log.debugv("Data size is " + str(len(data)))

            hist_type = row.histogram_collection[histogram_name].type

            # Collect all the data from this row according to the name of
            # the histogram
            joint_histogram['data'][row_name] = data

            log.debugv("The data of " + histogram_name + " is " + str(data))

            if hist_type == 'date':
                log.debug("Histogram " + histogram_name + " is type date")

                # Processing the dates to get the maximun and minimum month-year
                mindate = dt.datetime.fromtimestamp(min(data))
                maxdate = dt.datetime.fromtimestamp(max(data))
                bindate = dt.datetime(year=mindate.year, month=mindate.month, day=1)
                mybins = [bindate.timestamp()]
                while bindate < maxdate:
                    if bindate.month == 12:
                        bindate = dt.datetime(year=bindate.year + 1, month=1, day=1)
                    else:
                        bindate = dt.datetime(year=bindate.year, month=bindate.month + 1, day=1)
                    mybins.append(bindate.timestamp())
                mybins = mdates.epoch2num(mybins)

                plot_data = mdates.epoch2num(data)

                fig, ax = plt.subplots(1, 1, figsize=(300, 40), facecolor='white')
                # fig, ax = plt.subplots(1, 1, facecolor='white')

                ax.hist(plot_data, bins=mybins, ec='black')
                ax.set_title(histogram_name + " - " + row_name)
                ax.xaxis.set_major_locator(mdates.MonthLocator())
                ax.xaxis.set_major_formatter(mdates.DateFormatter('%m.%y'))
                fig.autofmt_xdate()
            elif hist_type == 'int':
                log.debug("Histogram " + histogram_name + " for " + row_name + " is type int")
                log.info("Histogram " + histogram_name + " for " + row_name + " is type int")
                fig, ax = plt.subplots(1, 1, figsize=(8, 6))

                # fig, ax = plt.subplots(1,1)
                # Ajust bins
                # max_exp = int(floor(loga(max(original_data), 10)))
                # binwidth = 10**(max_exp - 3)

                # TODO: parameterize binwidth
                binwidth = 10**5
                bins = np.arange(0, max(data) + binwidth, 2*binwidth)
                # bins = np.arange(100000, max(original_data) + binwidth, binwidth)

                # log.info("Max exp: " + str(max_exp))
                log.info("bindwidth: " + str(binwidth))
                log.info("Bins: " + str(bins))
                # ax.hist(data, bins=bins, ec='black')
                ax.hist(data, bins=bins)

                # TODO: argument option to calculate distribution
                # guessed_dist, params = gd.best_fit_distribution(data)

                # guessed_dist = st.gennorm
                # # my_params = (0.3324494702448755, 1808293.0000002861, 71602.6687589449)
                # params = guessed_dist.fit(data)

                # # log.info(my_params == params)
                # # log.info("my_params: " + str(my_params))
                # log.info("params:    " + str(params))

                # arg = params[:-2]
                # # loc: the mean of the distribution
                # loc = params[-2]
                # # scale: the standard deviation of the distribution
                # scale = params[-1]

                # my_rv = guessed_dist(*arg, loc=loc, scale=scale)

                # ax.hist(data, bins=bins, ec='black')

                # Option 1 (doesn't work)
                # x = np.linspace(0, 5000)
                # ax.plot(x, my_rv.pdf(x), 'r-', lw=2)

                # Option 2 ("scale" is not ajusted)
                # x = np.linspace(guessed_dist.ppf(0.01, *arg), guessed_dist.ppf(0.99, *arg), 5000)
                # # x = guessed_dist.rvs(*arg, 5000)
                # ax.plot(x, guessed_dist.pdf(x, *arg), 'r-', lw=5)

                # ax.plot(data, my_dist.pdf(data), 'r-', lw=5)
                # ax.plot(data, guessed_dist.pdf(data, *arg, loc=loc, scale=scale), 'r-', lw=5)
                # ax.plot(data, my_dist.pdf(data), 'r-', lw=5)

                # ax.plot(data, guessed_dist.pdf(data, *arg), 'g-', lw=5)

                # Start at left zero
                ax.set_xlim(left=0)

                @ticker.FuncFormatter
                def megas(x, pos):
                    return int(x/10**6)

                # The formatter for the labels in the ticks (ticks are the marks with numbers in the x axis)
                # ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: format(int(x), ',')))
                ax.xaxis.set_major_formatter(megas)

                # Where the major ticks should be
                # ax.xaxis.set_major_locator(ticker.MultipleLocator(binwidth))
                ax.xaxis.set_major_locator(ticker.MultipleLocator(10**6))

                # Rotate x ticks
                plt.xticks(rotation=45)

                # Adjust the lables if they go pass the figure
                plt.gcf().subplots_adjust(bottom=0.15)

                ax.set_title(histogram_name + " - " + row_name)
                plt.xlabel('APK size (in MB)')
                plt.ylabel('Number of APKs')

                plt.grid(linestyle="--")  # Grid in the histogram
            else:
                fig, ax = plt.subplots(1, 1)
                ax.hist(data, bins='auto', ec='black')
                ax.set_title(histogram_name + " - " + row_name)

            filename = histogram_name + "_" + row_name
            # plt.savefig(output_dir + "/histogram_"+ filename + ".png")
            # log.info("Histogram saved as histogram_"+ filename + ".png")
            plt.savefig(output_dir + "/histogram_" + filename + ".pdf")
            log.info("Histogram saved as histogram_" + filename + ".pdf")

            plt.figure().clear()
            plt.close(plt.figure())


            # Density graph
            if hist_type == 'int':
                log.debug("Density histogram " + histogram_name + " for " + row_name + " is type int")
                log.info("Density histogram " + histogram_name + " for " + row_name + " is type int")
                fig, ax = plt.subplots(1, 1, figsize=(8, 6))

                # fig, ax = plt.subplots(1,1)
                # Ajust bins
                # max_exp = int(floor(loga(max(original_data), 10)))
                # binwidth = 10**(max_exp - 3)

                # TODO: parameterize binwidth
                binwidth = 10**5
                bins = np.arange(0, max(data) + binwidth, 2*binwidth)
                # bins = np.arange(100000, max(original_data) + binwidth, binwidth)
                plt.style.use('seaborn-deep')
                # plt.gca().set_color_cycle(['blue', 'red', 'green', 'yellow'])
                # log.info("Max exp: " + str(max_exp))
                log.info("bindwidth: " + str(binwidth))
                log.info("Bins: " + str(bins))
                # ax.hist(data, bins=bins, ec='black')
                ax.hist(data, bins=bins, density=True)

                mn, mx = plt.xlim()
                # TODO: argument option to calculate distribution
                # guessed_dist, params = gd.best_fit_distribution(data)

                guessed_dist = st.gennorm
                params = (0.3324494702448755, 1808293.0000002861, 71602.6687589449)
                # params = guessed_dist.fit(data)

                # log.info(my_params == params)
                # log.info("my_params: " + str(my_params))
                log.info("params:    " + str(params))

                arg = params[:-2]
                # loc: the mean of the distribution
                loc = params[-2]
                # scale: the standard deviation of the distribution
                scale = params[-1]

                my_rv = guessed_dist(*arg, loc=loc, scale=scale)

                x = np.linspace(my_rv.ppf(0.01), my_rv.ppf(0.99), 5000)
                ax.plot(x, my_rv.pdf(x), 'r--', lw=0.5, label=guessed_dist.name)

                # Plotting KDE
                # for bandwidth in [1,0.1,0.05,0.01,0.005]:
                for bandwidth in [0.01]:
                    kde = st.gaussian_kde(data, bandwidth)
                    x = np.linspace(mn, mx, 5000)
                    ax.plot(x, kde.pdf(x), lw=0.5, label='KDE @ '+str(bandwidth))

                ax.legend(loc='upper right')
                # Start at left zero
                ax.set_xlim(left=0)

                @ticker.FuncFormatter
                def megas(x, pos):
                    return int(x/10**6)

                # The formatter for the labels in the ticks (ticks are the marks with numbers in the x axis)
                # ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: format(int(x), ',')))
                ax.xaxis.set_major_formatter(megas)

                # Where the major ticks should be
                # ax.xaxis.set_major_locator(ticker.MultipleLocator(binwidth))
                ax.xaxis.set_major_locator(ticker.MultipleLocator(10**6))

                # Rotate x ticks
                plt.xticks(rotation=45)

                # Adjust the lables if they go pass the figure
                plt.gcf().subplots_adjust(bottom=0.15)

                ax.set_title(histogram_name + " - " + row_name)
                plt.xlabel('APK size (in MB)')
                plt.ylabel('Probability')

                plt.grid(linestyle="--")  # Grid in the histogram

                filename = "density_" + histogram_name + "_" + row_name
                # plt.savefig(output_dir + "/histogram_"+ filename + ".png")
                # log.info("Histogram saved as histogram_"+ filename + ".png")
                plt.savefig(output_dir + "/" + filename + ".pdf")
                log.info("Histogram saved as density_histogram_" + filename + ".pdf")

                plt.figure().clear()
                plt.close(plt.figure())

            # print("Figure clear")

        # Draw joint histogram if there is more than one row (dataset)
        if len(datasets) > 1:

            # ax.hist(dataset_list, bins, label=['x', 'y'])
            label = []
            data = []
            mix_data = []

            for row_name in joint_histogram['data']:
                log.debug("Getting " + row_name)
                label.append(row_name)
                row_data = joint_histogram['data'][row_name]
                # print(str(row_name) + " is of type " + str(type(row_data)))
                data.append(row_data)
                mix_data.extend(row_data)

            log.debug("dataset has " + str(len(data)) + " entries")

            for num in data:
                # print("num is " + str(num))
                log.debug("The size of num is " + str(len(num)))

            s = "+"
            joint = s.join(label)

            if hist_type == 'date':
                log.debug("Histogram " + histogram_name + " is type date")
                # Processing the dates to get the maximun and minimum month-year
                mindate = dt.datetime.fromtimestamp(min(mix_data))
                maxdate = dt.datetime.fromtimestamp(max(mix_data))
                bindate = dt.datetime(year=mindate.year, month=mindate.month, day=1)
                mybins = [bindate.timestamp()]
                while bindate < maxdate:
                    if bindate.month == 12:
                        bindate = dt.datetime(year=bindate.year + 1, month=1, day=1)
                    else:
                        bindate = dt.datetime(year=bindate.year, month=bindate.month + 1, day=1)
                    mybins.append(bindate.timestamp())
                mybins = mdates.epoch2num(mybins)

                # plot_data = mdates.epoch2num(data)
                plot_data = []
                for list in data:
                    plot_data.append(mdates.epoch2num(list))

                fig, ax = plt.subplots(1,1, figsize=(200, 20), facecolor='white')
                # fig, ax = plt.subplots(1,1,facecolor='white')
                ax.hist(plot_data, bins=mybins, ec='black')
                log.debug("This title is " + histogram_name + " - " + joint)
                ax.set_title(histogram_name + " - " + joint)
                ax.xaxis.set_major_locator(mdates.MonthLocator())
                ax.xaxis.set_major_formatter(mdates.DateFormatter('%m.%y'))
                # fig.autofmt_xdate()

                # Changing the space between the ticks

                # plt.gca().margins(x=0)
                # plt.gcf().canvas.draw()
                tl = plt.gca().get_xticklabels()
                # log.debug("tl = " + str(tl))
                ticks_label = [t.get_window_extent().width for t in tl]
                log.debug(str(ticks_label))
                maxsize = max(ticks_label)

                # If the ticks label list brings 0 in everyting
                if maxsize == 0:
                    maxsize = 4
                log.debug("maxsize = " + str(maxsize))
                m = 0.2 # inch margin
                N = len(mix_data)
                log.debug("N = " + str(N))
                log.debug("plt.gcf().dpi = " + str(plt.gcf().dpi))
                s = maxsize/plt.gcf().dpi*N+2*m
                margin = m/plt.gcf().get_size_inches()[0]
                #
                # print("plt.gcf().get_size_inches()[1] = " + str(plt.gcf().get_size_inches()[1]))
                log.debug("plt.gcf().get_size_inches() = " + str(plt.gcf().get_size_inches()))
                log.debug("s = " + str(s))
                plt.gcf().subplots_adjust(left=margin, right=1.-margin)
                # plt.gcf().set_size_inches(s, 10)
                plt.gcf().set_size_inches(s*0.25, plt.gcf().get_size_inches()[1])

                fig.autofmt_xdate()

            elif hist_type == 'int':
                log.debug("Histogram " + histogram_name + " is type int")
                fig, ax = plt.subplots(1, 1, figsize=(200, 20))
                # fig, ax = plt.subplots(1,1)
                plt.style.use('seaborn-deep')

                max_exp = int(floor(loga(max(mix_data), 10)))
                binwidth = 10**(max_exp - 3)

                ax.hist(data, bins=np.arange(min(mix_data), max(mix_data) + binwidth, binwidth), label=label)
                ax.legend(loc='upper right')
                log.debug("This title is " + histogram_name + " - " + joint)
                ax.set_title(histogram_name + " - " + joint)
                ax.set_xlim(left=0)  # Start at left zero
                ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: format(int(x), ',')))  # The formatter for the labels in the ticks (ticks are the marks withc numbers in the x axis)
                ax.xaxis.set_major_locator(ticker.MultipleLocator(binwidth*10))
                plt.xticks(rotation=45)  # Rotate x ticks
                plt.gcf().subplots_adjust(bottom=0.15) # Adjust the lables if they go pass the figure
                plt.grid(linestyle="--")  # Grid in the histogram
            else:
                fig, ax = plt.subplots(1, 1)
                ax.hist(data, bins='auto', ec='black')
                log.debug("This title is " + histogram_name + " - " + joint)
                ax.set_title(histogram_name + " - " + joint)

            filename = histogram_name + "_" + joint
            # plt.savefig(output_dir + "/histogram_"+ filename + ".png")
            # log.info("Histogram saved as histogram_"+ filename + ".png")
            plt.savefig(output_dir + "/joint_histogram_" + filename + ".pdf")
            log.info("Histogram saved as joint_histogram_" + filename + ".pdf")
            plt.figure().clear()


def output_bars(datasets, bars_def, output_dir):

    # Pretty colors for the delight of the eye
    plt.style.use('seaborn-deep')

    for bar_name in bars_def:
        log.info("Processing bar " + bar_name)
        joint_bar = {'type': bars_def[bar_name][1], 'data': {}}

        for row_name in datasets:

            log.debugv("Assigning row " + row_name)
            row = datasets[row_name]

            # Print individual bars
            # Verify if there are bars to output
            # if len(row.bar_collection.keys()) != 0:
            #     for bar_name in row.bar_collection:
            #     def output_bars(data,output_dir):

            # TODO: if top in request, then
            for bar_name in row.bar_collection:
                bar = row.bar_collection[bar_name]

                top_num = 30
                counter = Counter(bar.data)
                top = counter.most_common(top_num)
                log.debug("This top has " + str(len(top)) + " elements")
                # print(top)
                bar.data = collections.OrderedDict(top)

                for i, key in enumerate(bar.data):
                    value = bar.data[key]
                    log.debug("Element: " + str(i) +" Key '" + str(key) + "' contains: " + str(value))

            data = row.bar_collection[bar_name].data

            # backup = []
            # original_data = data
            # for value in data:
            #     if value < (37 * 10**6):
            #         backup.append(value)
            # data = backup

            log.debugv("Data size is " + str(len(data)))

            bar_type = row.bar_collection[bar_name].type

            # Collect all the data from this row according to the name of
            # the bar
            joint_bar['data'][row_name] = data

            log.debugv("The data of " + bar_name + " is " + str(data))

            if bar_type == 'date':
                # TODO:
                log.debug("This is a date bar (which should be a histogram)")
            elif bar_type == 'int':
                log.debug("Bar " + bar_name + " for " + row_name + " is type int")
                # log.info("Bar " + bar_name + " for " + row_name + " is type int")
                fig, ax = plt.subplots(1, 1, figsize=(8, 8))
                # fig, ax = plt.subplots(1, 1, 'all')
                names = list(data)
                vals = data.values()

                # width = 0.7

                N = np.arange(len(data))

                rects = ax.bar(N, vals, align='center')

                # Add value to the bars
                for rect, label in zip(rects, vals):
                    height = rect.get_height()
                    label = '{:,}'.format(label)
                    ax.text(rect.get_x() + rect.get_width() / 1.5, height + 5, label,
                            ha='center', va='bottom', rotation=90, fontsize=7)

                # Adding extra y ticks
                yticks = list(plt.yticks()[0])
                steps = yticks[-1] - yticks[-2]
                last_tick = yticks[-1]
                extraticks = list(np.arange(yticks[-1], last_tick+2*steps, steps))
                plt.yticks(yticks + extraticks)

                # Add x ticks labels
                ax.set_xticks(N)
                ax.set_xticklabels(names)

                # Rotate x ticks labels
                for tick in ax.get_xticklabels():
                    tick.set_rotation(90)

                # Ajust the bottom of the subplot, to add space
                fig.subplots_adjust(bottom=0.35)

                # Thousands comma separator
                @ticker.FuncFormatter
                def thousand(x, pos):
                    return format(int(x), ',')
                ax.yaxis.set_major_formatter(thousand)

                # Change ticks' label size
                plt.tick_params(axis='both', which='major', labelsize=7)
                ax.set_title(bar_name + " - " + row_name)
                plt.xlabel('API names')
                plt.ylabel('Number of calls')

                # bottom, top = plt.ylim()
                # plt.ylim(bottom, top + 20)

                # plt.margins(0.2)
                # PLT.SUBPLOTS_ADJUST(BOTTOM=1.15)
                # plt.tight_layout(h_pad=1)
            else:
                # TODO:
                log.debug("Else")

            filename = bar_name + "_" + row_name
            # plt.savefig(output_dir + "/bar_"+ filename + ".png")
            # log.info("Bar saved as bar_"+ filename + ".png")
            plt.savefig(output_dir + "/bar_" + filename + ".pdf")
            log.info("Bar saved as bar_" + filename + ".pdf")

            plt.figure().clear()
            plt.close(plt.figure())

        # Draw joint bar if there is more than one row (dataset)
        if len(datasets) > 1:

            # ax.bar(dataset_list, bins, label=['x', 'y'])
            labels = []
            # mix_data = [[] for _ in range(len(datasets))]
            # mix_data = {}

            # Get the labels in a list
            for row_name in joint_bar['data']:
                log.debug("Getting " + row_name)
                row_data = joint_bar['data'][row_name]

                log.debug("Initially, " + row_name + " has a size of: " + str(len(data)))

                for num, label in enumerate(row_data):
                    if label not in labels:
                        labels.insert(num, label)

            mix_data = []
            zero_array = [0 for _ in range(len(labels))]

            for _ in range(len(datasets)):
                # Copy the array, instead of reference it
                mix_data.append(zero_array.copy())

            # Add values to the lists
            for row_num, row_name in enumerate(joint_bar['data']):
                row_data = joint_bar['data'][row_name]
                for label in row_data:
                    label_index = labels.index(label)
                    mix_data[row_num][label_index] = row_data[label]
                log.debug("mix_data for " + str(row_num) + " has " + str(len(mix_data[row_num])) + " elements")
                log.debug(str(mix_data[row_num]))

            for num, this_list in enumerate(mix_data):
                log.debug("list " + str(num) + " contains: " + str(this_list))

            log.debug("mix_data have length of " + str(len(mix_data)))
            log.debug("dataset has " + str(len(data)) + " entries")

            s = "+"
            joint = s.join(joint_bar['data'].keys()).replace(" ", "_")

            if bar_type == 'date':
                log.info("Bars with dates in it")
            elif bar_type == 'int':
                log.debug("Bar " + bar_name + " is type int")
                # fig, ax = plt.subplots(1,1)

                fig, ax = plt.subplots(1, 1, figsize=(8, 8))

                N = np.arange(len(labels))
                # N = np.linspace(1, len(labels) + 10, len(labels))

                width = 0.35
                widths = [0-width/2, width/2]

                # for num, row_name in enumerate(mix_data):
                #     ax.bar(N+widths[num], mix_data[row_name], align='center')
                for num in range(len(mix_data)):
                    rects = ax.bar(N+widths[num], mix_data[num], width, align='center')
                    for rect, label in zip(rects, mix_data[num]):
                        height = rect.get_height()
                        label = '{:,}'.format(label)
                        ax.text(rect.get_x() + rect.get_width() / 1.5, height + 5, label,
                                ha='center', va='bottom', rotation=90, fontsize=4)

                # Adding extra y ticks
                yticks = list(plt.yticks()[0])
                steps = yticks[-1] - yticks[-2]
                last_tick = yticks[-1]
                extraticks = list(np.arange(yticks[-1], last_tick+2*steps, steps))
                plt.yticks(yticks + extraticks)

                # Add x ticks labels
                ax.set_xticks(N)
                ax.set_xticklabels(labels)

                # Rotate x ticks labels
                for tick in ax.get_xticklabels():
                    tick.set_rotation(90)

                # Ajust the bottom of the subplot, to add space
                fig.subplots_adjust(bottom=0.45)
            else:
                fig, ax = plt.subplots(1, 1)
                ax.bar(data, bins='auto', ec='black')
                log.debug("This title is " + bar_name + " - " + joint)
                ax.set_title(bar_name + " - " + joint)

            filename = bar_name.replace(" ", "_") + "_" + joint
            # plt.savefig(output_dir + "/bar_"+ filename + ".png")
            # log.info("Bar saved as bar_"+ filename + ".png")
            plt.savefig(output_dir + "/joint_bar_" + filename + ".pdf")
            log.info("Bar saved as joint_bar_" + filename + ".pdf")
            plt.figure().clear()


def output_to_files(datasets, out_dir):
    log.info("Processing finished, outputing files")

    jsonfile = out_dir + "/" + OUTPUT_FILENAME + ".json"
    xlsxfile = out_dir + "/" + OUTPUT_FILENAME + ".xlsx"
    rawfile = out_dir + "/" + OUTPUT_FILENAME + "_raw.json"

    # Dictionary for the output JSON file
    dico = {}
    # Raw results of the histogram
    raw = {}
    row_num = 1

    # Initialize a workbook
    wb = Workbook()

    # Grab the active worksheet
    ws = wb.active

    # Create names for columns in workbook
    for row_name in datasets:
        row = datasets[row_name]
        ws.cell(row=row_num, column=2, value="Total")
        col_num = 3
        for column in row.columns:
            ws.cell(row=row_num, column=col_num, value=column.name)
            ws.merge_cells(start_row=row_num, start_column=col_num, end_row=row_num, end_column=col_num+1)

            col_num += 2

    row_num += 1
    col_num = 1

    # If the result JSON already exists, don't do anything
    # if not os.path.isfile(jsonfile):
    for row_name in datasets:
        log.debug("Assigning row " + row_name)

        dico[row_name] = {}
        raw[row_name] = {}

        row = datasets[row_name]

        # Assigning the name of the row
        ws.cell(row=row_num, column=col_num, value=row_name)
        col_num += 1

        # Assigning the total of the row
        ws.cell(row=row_num, column=col_num, value=row.total)
        ws.merge_cells(start_row=row_num, start_column=col_num, end_row=row_num+1, end_column=col_num)
        col_num += 1

        if len(row.columns) != 0:
            for column in row.columns:
                log.debug("Assiging column " + column.name)
                column.get_total()
                if column.req_type is None:
                    dico[row_name][column.name] = [column.total, column.pct_total, column.pct_depend]
                    ws.cell(row=row_num, column=col_num, value=column.total)
                    ws.cell(row=row_num+1, column=col_num, value=column.pct_total)
                    ws.cell(row=row_num+1, column=col_num+1, value=column.pct_depend)
                    # Grand total merge_cells
                    ws.merge_cells(start_row=row_num, start_column=col_num, end_row=row_num, end_column=col_num+1)
                else:
                    dico[row_name][column.name] = column.res_poll
                    ws.cell(row=row_num, column=col_num, value=str(column.res_poll)
                            .replace("{", "").replace("}", "").replace(",", "\n"))
                    ws.merge_cells(start_row=row_num, start_column=col_num, end_row=row_num+1, end_column=col_num+1)
                col_num += 2
            # Dataset name merge
            ws.merge_cells(start_row=row_num, start_column=1, end_row=row_num+1, end_column=1)
            row_num += 2
            col_num = 1
        # print(str(dico))
        if 'histograms' in row.histogram_collection.keys():
            for histogram_name in row.histogram_collection:
                # def output_histograms(data,output_dir):
                data = row.histogram_collection[histogram_name]['data']
                raw[row_name][histogram_name] = row.histogram_collection[histogram_name]

    # Create the output directory if it doesn't exists
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)

    with open(jsonfile, 'w') as out:
        json.dump(dico, out)
    log.info("File saved at " + jsonfile)

    with open(rawfile, 'w') as out:
        json.dump(raw, out)
    log.info("File saved at " + rawfile)

    # filename = "res.xlsx"
    wb.save(xlsxfile)
    log.info("File saved at " + xlsxfile)
    # else:
    #     log.warning("file already written. finishing")


def topN(dico, N, poll_type):
    if poll_type == 'top':
        res = dict(Counter(dico).most_common(N))
    elif poll_type == 'bottom' or poll_type == 'bot':
        res = dict(Counter(dico).most_common()[:-N-1:-1])
    return res

################################################################################
#                                                                              #
#                          Row and column definition                           #
#                                                                              #
################################################################################


class Bar:
    def __init__(self, req, type):
        self.request = req
        # self.req_parser = None
        self.poll = {}
        self.res_poll = {}
        # self.data = []
        self.data = {}
        self.type = type
        if len(self.request.split(":")) < 2:
            # Normal request
            self.req_type = None
            self.jpath = self.request.split(" ")[0]
            # self.req_parser = BooleanParser.BooleanParser(self.req)
        else:
            # This is poll request, it only needs to retrive a value
            # self.req_type can be top or bottom
            self.jpath, self.req_type, self.num_of_poll = self.request.split(":")
            self.num_of_poll = int(self.num_of_poll)


class Histogram:
    def __init__(self, req, type):
        self.request = req
        # self.req_parser = None
        self.poll = {}
        self.res_poll = {}
        self.data = []
        self.type = type
        if len(self.req.split(":")) < 2:
            # Normal request
            self.req_type = None
            self.jpath = self.req.split(" ")[0]
            # self.req_parser = BooleanParser.BooleanParser(self.req)
        else:
            # This is poll request, it only needs to retrive a value
            # self.req_type can be top or bottom
            self.jpath, self.req_type, self.num_of_poll = self.req.split(":")
            self.num_of_poll = int(self.num_of_poll)


class Column:
    def __init__(self, name, parent_row, req, depend=None):
        self.name = name
        self.total = self.pct_total = self.pct_depend = 0
        self.parent_row = parent_row
        self.req = req
        # self.req_parser = None
        self.poll = {}
        self.res_poll = {}
        # print("The req is " + str(self.req))
        # print("The length is " + str(len(self.req.split(":"))))
        if len(self.req.split(":")) < 2:
            # Normal request
            self.req_type = None
            self.jpath = self.req.split(" ")[0]
            self.req_parser = BooleanParser.BooleanParser(self.req)
        else:
            # This is poll request, it only needs to retrive a value
            # self.req_type can be top or bottom
            self.jpath, self.req_type, self.num_of_poll = self.req.split(":")
            self.num_of_poll = int(self.num_of_poll)
        self.depend = depend
        # print("Column " + self.name  + " created")

    def get_total(self):
        if self.req_type is None:
            # If there is no samples in the column, throw a warning
            if self.total == 0:
                log.warning(self.name + ": The total is 0, cannot divide by 0")
            else:
                # Calculate the percentage by total
                self.pct_total = (self.total / self.parent_row.total)*100
                # If no dependence is declared, the percentage is by total
                if self.depend is None:
                    self.pct_depend = self.pct_total
                else:
                    self.pct_depend = (self.total / self.depend.total)*100
        else:
            # print('getting poll')
            self.res_poll = topN(self.poll, self.num_of_poll, self.req_type)


class Row:
    var_dict = {}

    def __init__(self, name):
        self.name = name
        self.total = 0
        # self.
        self.columns = []
        self.histogram_collection = {}
        self.bar_collection = {}
        # print("Row " + self.name + " created")

    def create_column(self, name, req, depend_name=None):
        depend = None
        # If the name of the column for getting the percentage is different from None
        if depend_name is not None:
            # Check if the name exists:
            # Find the name in self.columns "column_name"
            # If the tuple is empty, the name was not found, depends is None
            column_name = (column for column in self.columns if column.name == depend_name)
            depend = next(column_name, None)
        # print('this depend is ' + str(depend))
        new_column = Column(name, self, req, depend)
        self.columns.append(new_column)

    def create_histogram(self, name, request, type):
        # self.histogram_collection[name] = {"type": type, "request": request, "data": []}
        new_histogram = Histogram(request, type)
        self.histogram_collection[name] = new_histogram

    def create_bar(self, name, request, type):
        # self.bar_collection[name] = {"type": type, "request": request, "data": []}
        new_bar = Bar(request, type)
        self.bar_collection[name] = new_bar

    def process(self, json_file):
        self.total += 1

        if len(self.columns) != 0:
            for column in self.columns:
                log.debugv('Processing data ' + str(self.total))
                log.debugv('parsing: ' + str(column.name))

                # Find the value of the JSONPath expression if it's not in the dictionary
                expression = column.jpath

                if expression not in self.var_dict.keys():
                    try:
                        parse = expression.split('.')
                        name = json_file['name']
                        log.debugv('the parsed list is: ' + str(parse))
                        log.debugv('My name is: ' + str(parse[2]))
                        self.var_dict[expression] = json_file[name][parse[2]][parse[3]]  # This may fail
                        if self.var_dict[expression] == "":
                            self.var_dict[expression] = 0
                    except Exception as e:
                        log.debugv("This happened during the parsing: " + str(e))
                        self.var_dict[expression] = None
                    # self.var_dict[expression] = parse(expression).find(json_file)
                log.debugv("This is the result of the parsing: " + str(self.var_dict[expression]))
                # If the expression's value is None (which it was not found in the JSON),
                # do nothing. Else, add the value
                if self.var_dict[expression] is not None:
                    # val = self.var_dict[expression][0].value
                    val = self.var_dict[expression]
                    log.debugv("The value for " + str(expression) + " is: " + str(val))
                    # Put some quotes to strings, so Jason can be happy :DDDD
                    if not isinstance(val, int) or isinstance(val, float):
                        log.debugv('There is an instance of string for val: ' + str(val))
                        val = "\"" + val + "\""
                    elif isinstance(val, bool):
                        log.debugv('There is an instance of bool for val: ' + str(val))
                        val = "\"" + str(val) + "\""
                    if column.req_type is None:
                        log.debugv('changing ' + column.jpath + ' -> ' + str(val))
                        evaluator = column.req_parser
                        log.debugv('The request to evaluate is: ' + str(evaluator.tokenizer.expression))
                        # {expression:val} => change 'expression' to 'val' when evaluating
                        res = evaluator.evaluate({expression: val})
                        log.debugv("The result was: " + str(res))
                        if res:
                            column.total += 1
                    else:
                        val = str(val)
                        if val in column.poll.keys():
                            column.poll[val] += 1
                        else:
                            column.poll[val] = 1

        if (self.histogram_collection.keys()) != 0:
            for histogram_name in self.histogram_collection:
                # histogram_dict = self.histogram_collection[histogram_name]
                histogram = self.histogram_collection[histogram_name]

                # Find the value of the JSONPath expression if it's not in the dictionary
                # expression = histogram_dict['request']
                expression = histogram.request

                # Check if is value, dict, or list

                if expression not in self.var_dict.keys():
                    try:
                        name = json_file['name']
                        parse = expression.split('.')
                        for i, expr in enumerate(parse):
                            if i == 1:
                                expression_val = json_file[name]
                            else:
                                expression_val = expression[expr]
                        log.debugv('the parsed list is: ' + str(parse))
                        # log.debugv('My name is: ' + str(parse[2]))
                        expression_class = expression_val.__class__
                        if expression_class == int or expression_class == float or expression_class == str \
                                or expression_class == bool:
                            # This may fail
                            # self.var_dict[expression] = json_file[name][parse[2]][parse[3]]
                            self.var_dict[expression] = expression_val
                            if histogram.type == 'date' and self.var_dict[expression] == "":
                                log.debugv('This date "' + expression + '" is now 0')
                                self.var_dict[expression] = 0
                        elif expression_class == dict:
                            print(expression + " is a dictionnary")

                        elif expression_class == list:
                            print(expression + " is a list")

                    except Exception as e:
                        if histogram.type == 'data':
                            self.var_dict[expression] = 0
                        else:
                            self.var_dict[expression] = None
                log.debugv("--- Histogram: This is the result of the parsing: " + str(self.var_dict[expression]))

                # Add the value if it exists
                if self.var_dict[expression] is not None:
                    val = self.var_dict[expression]
                    log.debugv("The value for " + str(expression) + " is: " + str(val))
                    # if histogram_dict['type'] == 'int':
                    if histogram.req_type == 'int':
                        # histogram_dict['data'].append(int(val))
                        histogram.data.append(int(val))
                    else:
                        # histogram_dict['data'].append(val)
                        histogram.data.append(val)
        else:
            log.debugv("No histograms, moving on")

        # Moving to bars ================================================================================
        if (self.bar_collection.keys()) != 0:
            # print("hello")
            for bar_name in self.bar_collection:
                # bar_dict = self.bar_collection[bar_name]
                bar = self.bar_collection[bar_name]

                # Find the value of the JSONPath expression if it's not in the dictionary
                # expression = bar_dict['request']
                expression = bar.request

                # Check if is value, dict, or list
                log.debugv("Bar request: " + str(expression))

                if expression not in self.var_dict.keys():
                    # try:
                    name = json_file['name']
                    parse = expression.split('.')
                    request_tail = ""
                    log.debugv('the parsed list is: ' + str(parse))
                    for i, expr in enumerate(parse):
                        log.debugv("parse number: " + str(i) + ", expr is: " + str(expr))
                        if i == 1:
                            expression_val = json_file[name]
                        elif i > 1:
                            if expr == '*':
                                request_tail = parse[i+1]
                                break
                            else:
                                expression_val = expression_val[expr]
                    # log.debugv('My name is: ' + str(parse[2]))
                    expression_class = expression_val.__class__
                    log.debugv('expression is of ' + str(expression_class))
                    if expression_class == int or expression_class == float or expression_class == str \
                            or expression_class == bool:
                        # This may fail
                        # self.var_dict[expression] = json_file[name][parse[2]][parse[3]]
                        self.var_dict[expression] = expression_val
                        if bar.type == 'date' and self.var_dict[expression] == "":
                            log.debugv('This date "' + expression + '" is now 0')
                            self.var_dict[expression] = 0
                    # As the expression gets a dict, add the key/values for all according to the last key in the request
                    elif expression_class == dict:
                        log.debugv(expression + " is a dictionnary")
                        log.debugv("The request tail is " + str(request_tail))
                        # Add keys and values directly to dictionnary
                        log.debugv("This dictonnary contains: " + str(expression_val))
                        # For each key, add the value
                        for key in expression_val:
                            # Skip status
                            if key == "status":
                                continue
                            value = expression_val[key][request_tail]
                            log.debugv("The key is:" + str(key) + "and the value is: " + str(value))

                            if key in self.var_dict:
                                self.var_dict[key] += value
                            else:
                                self.var_dict[key] = value
                    elif expression_class == list:
                        log.debugv(expression + " is a list")

                    # except Exception as e:
                    #     log.warning("An excepcion occured: " + str(e))
                    #     if bar.type == 'data':
                    #         self.var_dict[expression] = 0
                    #     else:
                    #         self.var_dict[expression] = None
                # log.debugv("--- Bar: This is the result of the parsing: " + str(self.var_dict))

            bar.data = Counter(bar.data) + Counter(self.var_dict)
            # log.info("bar data is of " + str(bar.data.__class__))
                # Add the value if it exists
                # if self.var_dict[expression] is not None:
                #     val = self.var_dict[expression]
                #     log.debugv("The value for " + str(expression) + " is: " + str(val))
                #     # if bar_dict['type'] == 'int':
                #     if bar.req_type == 'int':
                #         # bar_dict['data'].append(int(val))
                #         bar.data.append(int(val))
                #     else:
                #         # bar_dict['data'].append(val)
                #         bar.data.append(val)
        else:
            log.debugv("No bars, moving on")

        # time.sleep(1)
        # Reinitialize the dictionary
        self.var_dict = {}

################################################################################
#                                                                              #
#                  postprocessing function (primary function)                  #
#                                                                              #
################################################################################


def postprocessing(myjsonconfig, verbose=0):

    # print('This verbosity is ' + str(verbose))
    # quit()
    #
    # logSetup(verbose)

    t_start = time.time()
    filename = myjsonconfig

    # Check if the JSON config file exists
    if not os.path.isfile(filename):
        log.warning("The file " + filename + " doesn't exist. Quitting")
    else:
        with open(filename) as config:
            myjson = json.load(config)

        log.debugv("This is the config file: " + str(myjson))

        log.info("Output dir: " + myjson['output_dir'])
        # jsonfile = myjson['output_dir'] + "/" + OUTPUT_FILENAME + ".json"

        # Initialize Row and Columns objects
        datasets = {}  # An empty dataset (dictionary)

        # for each row, because they are different datasets
        for row in myjson['rows']:

            log.info('Processing ' + row)

            # create row object
            datasets[row] = Row(row)

            # create columns
            if 'columns' in myjson.keys():
                for column in myjson['columns']:
                    log.debugv("the column is " + str(column))
                    # create_column(self,name,req,depends=None):
                    datasets[row].create_column(column, myjson['columns'][column][0], myjson['columns'][column][1])
            else:
                log.debugv("There are no columns in the config file, moving on")

            # Create histograms
            if 'histograms' in myjson.keys():
                for histogram in myjson['histograms']:
                    log.debugv("Adding " + histogram + " histogram")
                    # Each row will contain the same values: name, request (JSONPath), and type
                    histogram_request = myjson['histograms'][histogram][0]
                    histogram_type = myjson['histograms'][histogram][1]
                    datasets[row].create_histogram(histogram, histogram_request, histogram_type)
            else:
                log.debugv("There are no histograms in the config file, moving on")

            # Create bars
            if 'bars' in myjson.keys():
                for bar in myjson['bars']:
                    log.debugv("Adding " + bar + " bar")
                    # Each row will contain the same values: name, request (JSONPath), and type
                    bar_request = myjson['bars'][bar][0]
                    bar_type = myjson['bars'][bar][1]
                    datasets[row].create_bar(bar, bar_request, bar_type)
            else:
                log.debugv("There are no bars in the config file, moving on")

            # TODO: If dataset dir doesn't exists, continue
            mypath = myjson['rows'][row]

            try:
                files = [f for f in listdir(mypath) if isfile(join(mypath, f)) and f.endswith(".json")]
            except Exception:
                raise Exception("Cannot open dir")
                continue

            # Process each file into the rows
            numFiles = len(files)
            log.info("Processing " + str(numFiles) + " files")
            for filename in files:

                log.debugv(row + "$ Processing file number " + str(numFiles) + ": " + filename + " JSON file")
                numFiles -= 1
                with open(mypath + "/" + filename) as f:
                    mwjson = json.load(f)

                # Get the name of the file (without the extension)
                mwjson['name'] = filename.split('.')[0]
                datasets[row].process(mwjson)

            if verbose >= 1:
                if 'histograms' in myjson.keys():
                    for histo in myjson['histograms']:
                        log.debug("Dataset " + row + " has " + str(len(datasets[row].histogram_collection[histo].data)) + " entries in " + histo)

        output_to_files(datasets, myjson['output_dir'])

        t_end = time.time()
        log.info("PROCESS TIME: " + str(round(t_end - t_start, 1)) + " s")

        if 'histograms' in myjson.keys():
            if verbose == 2:
                for name in datasets:
                    log.debugv("This dataset has evaludated " + str(datasets[name].total) + " entries")
                    for histo in myjson['histograms']:
                        log.debugv("Dataset " + datasets[name].name + " has " + str(len(datasets[name].histogram_collection[histo]['data'])) + " entries in " + histo)

            output_histograms(datasets, myjson['histograms'], myjson['output_dir'])

        if 'bars' in myjson.keys():
            output_bars(datasets, myjson['bars'], myjson['output_dir'])

    t_end = time.time()
    log.info("TOTAL TIME: " + str(round(t_end - t_start,1)) + " s")
    quit()


if __name__ == "__main__":
    # print("your arguments are: " + str(sys.argv))
    # Doc : https://docs.python.org/3/howto/argparse.html#id1
    parser = argparse.ArgumentParser(description="PostProcessing, for json files generated by the orchestrator.")
    parser.add_argument('json_config_file', help='The path to the JSON config file')
    parser.add_argument('-s', help='Optrion for StatsInvoke')
    parser.add_argument('-v', help='Output information to the standart output (-vv is very verbose)', action="count")

    # parser.add_argument('-H', metavar='Result JSON file', help='Output information to the standart output (-vv is very verbose)')

    args = parser.parse_args()

    if args.v is not None:
        verbosity = args.v
    else:
        verbosity = 0

    logSetup(verbosity)
    # log.info('This verbosity is ' + str(verbosity))
    # quit()
    postprocessing(args.json_config_file, verbosity)

    quit()
