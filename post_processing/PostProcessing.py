import os
import time
import logging  # Log info output
import datetime
import argparse  # Program argument parser
import json
import collections  # imported for OrderedDict
# import math
import copy

from os import listdir
from os.path import isfile, join

import BooleanParser

import sys

from collections import Counter # For getting top or botton N elements from a list
from openpyxl import Workbook   # For outputing results in xlsx (Excel format)
import datetime as dt

import numpy as np
# import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.ticker as ticker
# import guess_distribution as gd
import scipy.stats as st

from math import floor, log as loga
"""
Usage:
    python3 <this_file> <json_config>

where:
    json_config: The file that holds the definitions of the rows and columns to be processed

Example of config file:

see `post_processing/examples/drebin.json`

    {
        "rows":{
            "Dataset 1":"/path/to/dataset/1",  ----> Directories that contain JSON
            "Dataset 2":"/path/to/dataset/2",        files generated by the orchestrator
            "Dataset 3":"/path/to/dataset/3",
        },
        "columns":{
            "Column 1":[
                "<boolean expression with JSONPath>",
                null        ---------------------------> No special porcentage is done
            ],
            "Column 2":[
                "<boolean expression with JSONPath>",
                null
            ],
            "Column 3":[
                "<boolean expression with JSONPath>",
                "Column 2"               ------------> Porcentage in relation to Column 2,
            ]                                          which MUST be declared before
        },
        "histograms":{
            "Data 1":[
    			"<JSONPath expression>",
    			"<type>"             ---------------> This can be date, int, float, string
    		],
    		"Data 2":[
    			"<JSONPath expression>",
    			"<type>"
    		]
        },
        "output_dir":"/path/to/your/output/dir"
    }


The JSONPath implementation is from (https://github.com/h2non/jsonpath-ng)

The boolean expression parser implementation is from (https://gist.github.com/leehsueh/1290686)

It follows this grammar:

    Expression --> Terminal (>,<,>=,<=,==) Terminal
    Terminal --> Number or String or Variable

To compare a variable to a string, they must be enclosed in <""> in the JSON file. For example:

    $..Unzip.status == \"done\"

This expression compares if the variable `$..Unzip.status` is equal to the string `done`.

For our purposes, all the JSONPath expressions are variables.

### TODO:
* [ ]  Expand expression for multiple requests
"""

# Name without extension
OUTPUT_FILENAME = "res"
# Adds a very verbose level of logs
DEBUG_LEVELV_NUM = 9
logging.addLevelName(DEBUG_LEVELV_NUM, "DEBUGV")


def debugv(self, message, *args, **kws):
    # Yes, logger takes its '*args' as 'args'.
    if self.isEnabledFor(DEBUG_LEVELV_NUM):
        self._log(DEBUG_LEVELV_NUM, message, args, **kws)


logging.Logger.debugv = debugv
log = logging.getLogger("post-processing")


# Tries to apply colors to logs
def applyColorsToLogs():
    try:
        import coloredlogs

        style = coloredlogs.DEFAULT_LEVEL_STYLES
        style['debugv'] = {'color': 'magenta'}
        coloredlogs.install(
            show_hostname=False, show_name=True,
            logger=log,
            level=DEBUG_LEVELV_NUM,
            fmt='%(asctime)s [%(levelname)8s] %(message)s'
            # Default format:
            # fmt='%(asctime)s %(hostname)s %(name)s[%(process)d] %(levelname)s
            #  %(message)s'
        )
    except ImportError:
        log.error("Can't import coloredlogs, logs may not appear correctly.")


def logSetup(level):
    # if -vv option as program argument
    if level == 2:
        log.setLevel(DEBUG_LEVELV_NUM)
        log.info("Debug is Very Verbose.")
    # if -v option as program argument
    elif level == 1:
        log.setLevel(logging.DEBUG)
        log.info("Debug is Verbose.")
    # if no (-v) option
    elif level == 0:
        log.setLevel(logging.INFO)
        log.info("Debug is Normal.")
    else:
        # else
        log.setLevel(logging.INFO)
        log.warning("Logging level \"{}\" not defined, setting \"normal\" instead"
                    .format(level))


applyColorsToLogs()

################################################################################
#                                                                              #
#                             Function definitions                             #
#                                                                              #
################################################################################


def epoch_to_date(epoch):
    return datetime.datetime.fromtimestamp(epoch).strftime('%Y-%m-%d %H:%M:%S')


os.stat_float_times(False)


def output_histograms(datasets, histograms_def, output_dir):

    # Pretty colors for the delight of the eye
    plt.style.use('seaborn-deep')

    # joint_histogram = {}
    # for histogram in histograms_def:
    #     joint_histogram[name] = {'type':histograms_def[1], 'data'=[]}

    for histogram_name in histograms_def:
        log.info("Processing histogram " + histogram_name)
        # joint_histogram = {}
        joint_histogram = {'type': histograms_def[histogram_name][1], 'data': {}}

        # rows = []

        for row_name in datasets:

            # log.debug("Assigning row " + row_name)
            log.debugv("Assigning row " + row_name)
            row = datasets[row_name]

            # Print individual histograms
            # Verify if there are histograms to output
            # if len(row.histogram_collection.keys()) != 0:
            #     for histogram_name in row.histogram_collection:
            #     def output_histograms(data,output_dir):

            data = row.histogram_collection[histogram_name].data

            # backup = []
            # original_data = data
            # for value in data:
            #     if value < (37 * 10**6):
            #         backup.append(value)
            # data = backup

            log.debugv("Data size is " + str(len(data)))

            hist_type = row.histogram_collection[histogram_name].type

            # Collect all the data from this row according to the name of
            # the histogram
            joint_histogram['data'][row_name] = data

            log.debugv("The data of " + histogram_name + " is " + str(data))

            if hist_type == 'date':
                log.debug("Histogram " + histogram_name + " is type date")

                # Processing the dates to get the maximun and minimum month-year
                mindate = dt.datetime.fromtimestamp(min(data))
                maxdate = dt.datetime.fromtimestamp(max(data))
                bindate = dt.datetime(year=mindate.year, month=mindate.month, day=1)
                mybins = [bindate.timestamp()]
                while bindate < maxdate:
                    if bindate.month == 12:
                        bindate = dt.datetime(year=bindate.year + 1, month=1, day=1)
                    else:
                        bindate = dt.datetime(year=bindate.year, month=bindate.month + 1, day=1)
                    mybins.append(bindate.timestamp())
                mybins = mdates.epoch2num(mybins)

                plot_data = mdates.epoch2num(data)

                fig, ax = plt.subplots(1, 1, figsize=(300, 40), facecolor='white')
                # fig, ax = plt.subplots(1, 1, facecolor='white')

                ax.hist(plot_data, bins=mybins, ec='black')
                ax.set_title(histogram_name + " - " + row_name)
                ax.xaxis.set_major_locator(mdates.MonthLocator())
                ax.xaxis.set_major_formatter(mdates.DateFormatter('%m.%y'))
                fig.autofmt_xdate()
            elif hist_type == 'int':
                log.debug("Histogram " + histogram_name + " for " + row_name + " is type int")
                log.info("Histogram " + histogram_name + " for " + row_name + " is type int")
                fig, ax = plt.subplots(1, 1, figsize=(8, 6))

                # fig, ax = plt.subplots(1,1)
                # Ajust bins
                # max_exp = int(floor(loga(max(original_data), 10)))
                # binwidth = 10**(max_exp - 3)

                # TODO: parameterize binwidth
                binwidth = 10**5
                bins = np.arange(0, max(data) + binwidth, 2*binwidth)
                # bins = np.arange(100000, max(original_data) + binwidth, binwidth)

                # log.info("Max exp: " + str(max_exp))
                log.info("bindwidth: " + str(binwidth))
                log.info("Bins: " + str(bins))
                # ax.hist(data, bins=bins, ec='black')
                ax.hist(data, bins=bins)

                # TODO: argument option to calculate distribution
                # guessed_dist, params = gd.best_fit_distribution(data)

                # guessed_dist = st.gennorm
                # # my_params = (0.3324494702448755, 1808293.0000002861, 71602.6687589449)
                # params = guessed_dist.fit(data)

                # # log.info(my_params == params)
                # # log.info("my_params: " + str(my_params))
                # log.info("params:    " + str(params))

                # arg = params[:-2]
                # # loc: the mean of the distribution
                # loc = params[-2]
                # # scale: the standard deviation of the distribution
                # scale = params[-1]

                # my_rv = guessed_dist(*arg, loc=loc, scale=scale)

                # ax.hist(data, bins=bins, ec='black')

                # Option 1 (doesn't work)
                # x = np.linspace(0, 5000)
                # ax.plot(x, my_rv.pdf(x), 'r-', lw=2)

                # Option 2 ("scale" is not ajusted)
                # x = np.linspace(guessed_dist.ppf(0.01, *arg), guessed_dist.ppf(0.99, *arg), 5000)
                # # x = guessed_dist.rvs(*arg, 5000)
                # ax.plot(x, guessed_dist.pdf(x, *arg), 'r-', lw=5)

                # ax.plot(data, my_dist.pdf(data), 'r-', lw=5)
                # ax.plot(data, guessed_dist.pdf(data, *arg, loc=loc, scale=scale), 'r-', lw=5)
                # ax.plot(data, my_dist.pdf(data), 'r-', lw=5)

                # ax.plot(data, guessed_dist.pdf(data, *arg), 'g-', lw=5)

                # Start at left zero
                ax.set_xlim(left=0)

                @ticker.FuncFormatter
                def megas(x, pos):
                    return int(x/10**6)

                # The formatter for the labels in the ticks (ticks are the marks with numbers in the x axis)
                # ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: format(int(x), ',')))
                ax.xaxis.set_major_formatter(megas)

                # Where the major ticks should be
                # ax.xaxis.set_major_locator(ticker.MultipleLocator(binwidth))
                ax.xaxis.set_major_locator(ticker.MultipleLocator(10**6))

                # Rotate x ticks
                plt.xticks(rotation=45)

                # Adjust the lables if they go pass the figure
                plt.gcf().subplots_adjust(bottom=0.15)

                ax.set_title(histogram_name + " - " + row_name)
                plt.xlabel('APK size (in MB)')
                plt.ylabel('Number of APKs')

                plt.grid(linestyle="--")  # Grid in the histogram
            else:
                fig, ax = plt.subplots(1, 1)
                ax.hist(data, bins='auto', ec='black')
                ax.set_title(histogram_name + " - " + row_name)

            filename = histogram_name + "_" + row_name
            # plt.savefig(output_dir + "/histogram_"+ filename + ".png")
            # log.info("Histogram saved as histogram_"+ filename + ".png")
            plt.savefig(output_dir + "/histogram_" + filename + ".pdf")
            log.info("Histogram saved as histogram_" + filename + ".pdf")

            plt.figure().clear()
            plt.close(plt.figure())

            # Density graph
            if hist_type == 'int':
                log.debug("Density histogram " + histogram_name + " for " + row_name + " is type int")
                log.info("Density histogram " + histogram_name + " for " + row_name + " is type int")
                fig, ax = plt.subplots(1, 1, figsize=(8, 6))

                # fig, ax = plt.subplots(1,1)
                # Ajust bins
                # max_exp = int(floor(loga(max(original_data), 10)))
                # binwidth = 10**(max_exp - 3)

                # TODO: parameterize binwidth
                binwidth = 10**5
                bins = np.arange(0, max(data) + binwidth, 2*binwidth)
                # bins = np.arange(100000, max(original_data) + binwidth, binwidth)
                plt.style.use('seaborn-deep')
                # plt.gca().set_color_cycle(['blue', 'red', 'green', 'yellow'])
                # log.info("Max exp: " + str(max_exp))
                log.info("bindwidth: " + str(binwidth))
                log.info("Bins: " + str(bins))
                # ax.hist(data, bins=bins, ec='black')
                ax.hist(data, bins=bins, density=True)

                mn, mx = plt.xlim()
                # TODO: argument option to calculate distribution
                # guessed_dist, params = gd.best_fit_distribution(data)

                guessed_dist = st.gennorm
                params = (0.3324494702448755, 1808293.0000002861, 71602.6687589449)
                # params = guessed_dist.fit(data)

                # log.info(my_params == params)
                # log.info("my_params: " + str(my_params))
                log.info("params:    " + str(params))

                arg = params[:-2]
                # loc: the mean of the distribution
                loc = params[-2]
                # scale: the standard deviation of the distribution
                scale = params[-1]

                my_rv = guessed_dist(*arg, loc=loc, scale=scale)

                x = np.linspace(my_rv.ppf(0.01), my_rv.ppf(0.99), 5000)
                ax.plot(x, my_rv.pdf(x), 'r--', lw=0.5, label=guessed_dist.name)

                # Plotting KDE
                # for bandwidth in [1,0.1,0.05,0.01,0.005]:
                for bandwidth in [0.01]:
                    kde = st.gaussian_kde(data, bandwidth)
                    x = np.linspace(mn, mx, 5000)
                    ax.plot(x, kde.pdf(x), lw=0.5, label='KDE @ '+str(bandwidth))

                ax.legend(loc='upper right')
                # Start at left zero
                ax.set_xlim(left=0)

                @ticker.FuncFormatter
                def megas(x, pos):
                    return int(x/10**6)

                # The formatter for the labels in the ticks (ticks are the marks with numbers in the x axis)
                # ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: format(int(x), ',')))
                ax.xaxis.set_major_formatter(megas)

                # Where the major ticks should be
                # ax.xaxis.set_major_locator(ticker.MultipleLocator(binwidth))
                ax.xaxis.set_major_locator(ticker.MultipleLocator(10**6))

                # Rotate x ticks
                plt.xticks(rotation=45)

                # Adjust the lables if they go pass the figure
                plt.gcf().subplots_adjust(bottom=0.15)

                ax.set_title(histogram_name + " - " + row_name)
                plt.xlabel('APK size (in MB)')
                plt.ylabel('Probability')

                plt.grid(linestyle="--")  # Grid in the histogram

                filename = "density_" + histogram_name + "_" + row_name
                # plt.savefig(output_dir + "/histogram_"+ filename + ".png")
                # log.info("Histogram saved as histogram_"+ filename + ".png")
                plt.savefig(output_dir + "/" + filename + ".pdf")
                log.info("Histogram saved as density_histogram_" + filename + ".pdf")

                plt.figure().clear()
                plt.close(plt.figure())

            # print("Figure clear")

        # Draw joint histogram if there is more than one row (dataset)
        if len(datasets) > 1:

            label = []
            data = []  # This contains the lists of data for each dataset
            mix_data = []  # The sum of all the data in the datasets

            for row_name in joint_histogram['data']:
                log.debug("Getting " + row_name)
                label.append(row_name)
                row_data = joint_histogram['data'][row_name]
                # print(str(row_name) + " is of type " + str(type(row_data)))
                data.append(row_data)
                mix_data.extend(row_data)

            log.debug("dataset has " + str(len(data)) + " entries")

            for num in data:
                # print("num is " + str(num))
                log.debug("The size of num is " + str(len(num)))

            s = "+"
            joint = s.join(label)

            if hist_type == 'date':
                log.debug("Histogram " + histogram_name + " is type date")
                # Processing the dates to get the maximun and minimum month-year
                mindate = dt.datetime.fromtimestamp(min(mix_data))
                maxdate = dt.datetime.fromtimestamp(max(mix_data))
                bindate = dt.datetime(year=mindate.year, month=mindate.month, day=1)
                mybins = [bindate.timestamp()]
                while bindate < maxdate:
                    if bindate.month == 12:
                        bindate = dt.datetime(year=bindate.year + 1, month=1, day=1)
                    else:
                        bindate = dt.datetime(year=bindate.year, month=bindate.month + 1, day=1)
                    mybins.append(bindate.timestamp())
                mybins = mdates.epoch2num(mybins)

                # plot_data = mdates.epoch2num(data)

                plot_data = []
                for list in data:
                    plot_data.append(mdates.epoch2num(list))

                fig, ax = plt.subplots(1,1, figsize=(200, 20), facecolor='white')
                # fig, ax = plt.subplots(1,1,facecolor='white')
                ax.hist(plot_data, bins=mybins, ec='black')
                log.debug("This title is " + histogram_name + " - " + joint)
                ax.set_title(histogram_name + " - " + joint)
                ax.xaxis.set_major_locator(mdates.MonthLocator())
                ax.xaxis.set_major_formatter(mdates.DateFormatter('%m.%y'))
                # fig.autofmt_xdate()

                # Changing the space between the ticks

                # plt.gca().margins(x=0)
                # plt.gcf().canvas.draw()
                tl = plt.gca().get_xticklabels()
                # log.debug("tl = " + str(tl))
                ticks_label = [t.get_window_extent().width for t in tl]
                log.debug(str(ticks_label))
                maxsize = max(ticks_label)

                # If the ticks label list brings 0 in everyting
                if maxsize == 0:
                    maxsize = 4
                log.debug("maxsize = " + str(maxsize))
                m = 0.2  # inch margin
                N = len(mix_data)
                log.debug("N = " + str(N))
                log.debug("plt.gcf().dpi = " + str(plt.gcf().dpi))
                s = maxsize/plt.gcf().dpi*N+2*m
                margin = m/plt.gcf().get_size_inches()[0]
                #
                # print("plt.gcf().get_size_inches()[1] = " + str(plt.gcf().get_size_inches()[1]))
                log.debug("plt.gcf().get_size_inches() = " + str(plt.gcf().get_size_inches()))
                log.debug("s = " + str(s))
                plt.gcf().subplots_adjust(left=margin, right=1.-margin)
                # plt.gcf().set_size_inches(s, 10)
                plt.gcf().set_size_inches(s*0.25, plt.gcf().get_size_inches()[1])

                fig.autofmt_xdate()

            elif hist_type == 'int':
                log.debug("Histogram " + histogram_name + " is type int")
                fig, ax = plt.subplots(1, 1, figsize=(200, 20))
                # fig, ax = plt.subplots(1,1)
                plt.style.use('seaborn-deep')

                max_exp = int(floor(loga(max(mix_data), 10)))
                binwidth = 10**(max_exp - 3)

                ax.hist(data, bins=np.arange(min(mix_data), max(mix_data) + binwidth, binwidth), label=label)
                ax.legend(loc='upper right')
                log.debug("This title is " + histogram_name + " - " + joint)
                ax.set_title(histogram_name + " - " + joint)

                # Start at left zero
                ax.set_xlim(left=0)

                # The formatter for the labels in the ticks (ticks are the marks withc numbers in the x axis)
                ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: format(int(x), ',')))
                ax.xaxis.set_major_locator(ticker.MultipleLocator(binwidth*10))

                # Rotate x ticks
                plt.xticks(rotation=45)

                # Adjust the lables if they go pass the figure
                plt.gcf().subplots_adjust(bottom=0.15)
                plt.grid(linestyle="--")  # Grid in the histogram
            else:
                fig, ax = plt.subplots(1, 1)
                ax.hist(data, bins='auto', ec='black')
                log.debug("This title is " + histogram_name + " - " + joint)
                ax.set_title(histogram_name + " - " + joint)

            filename = histogram_name + "_" + joint
            # plt.savefig(output_dir + "/histogram_"+ filename + ".png")
            # log.info("Histogram saved as histogram_"+ filename + ".png")
            plt.savefig(output_dir + "/joint_histogram_" + filename + ".pdf")
            log.info("Histogram saved as joint_histogram_" + filename + ".pdf")
            plt.figure().clear()


def output_bars(datasets, bars_def, output_dir):

    # Pretty colors for the delight of the eye
    plt.style.use('seaborn-deep')

    # TODO: Specify in config file:
    # color
    # hatchs
    # bar width
    # top or botom number
    # switch for distribution graph

    # hatchs = ['/', '.', 'x']
    # hatchs = ['/', '.']
    # orig_datasets = datasets.copy()
    orig_datasets = copy.deepcopy(datasets)
    # orig_datasets = {}

    bar_ylabel = "Number of invoked methods"
    bar_xlabel = "API Names"

    for bar_num, bar_name in enumerate(bars_def):
        log.info("Processing bar " + bar_name)
        joint_bar = {'type': bars_def[bar_name][1], 'data': {}}

        top_data = {}

        for row_name in datasets:

            log.debugv("Assigning row " + row_name)
            row = datasets[row_name]

            # top_data[row_name] = {}
            # row_top_data = top_data[row_name]

            # Initializing top data
            # top_data = {}
            # Print individual bars
            # Verify if there are bars to output
            # if len(row.bar_collection.keys()) != 0:
            #     for bar_name in row.bar_collection:
            #     def output_bars(data,output_dir):

            # data_orig = {}

            # TODO: if top in request, then
            # for bar_name2 in row.bar_collection:
            bar = row.bar_collection[bar_name]
            # data_orig[bar_name] = bar.data.copy()
            log.debug("bar " + str(bar_name) + " has " + str(len(bar.data)) + " elements at the beginning")
            # time.sleep(1)

            # top_num = 30
            top_num = 10
            counter = Counter(bar.data)
            top = counter.most_common(top_num)
            log.debug("Top of " + str(bar_name) + " has " + str(len(top)) + " elements")
            # print(top)
            # row_top_data[bar_name] = collections.OrderedDict(top)
            top_data[row_name] = collections.OrderedDict(top)

            for i, key in enumerate(bar.data):
                value = bar.data[key]
                log.debugv("Element: " + str(i) + " Key '" + str(key) + "' contains: " + str(value))

            # data = row.bar_collection[bar_name].data
            # data = row_top_data[bar_name]
            data = top_data[row_name]

            # backup = []
            # original_data = data
            # for value in data:
            #     if value < (37 * 10**6):
            #         backup.append(value)
            # data = backup

            log.debugv("Data size is " + str(len(data)))

            bar_type = row.bar_collection[bar_name].type

            # Collect all the data from this row according to the name of
            # the bar
            joint_bar['data'][row_name] = data

            log.debugv("The data of " + bar_name + " is " + str(data))

            if bar_type == 'date':
                # TODO:
                log.debug("This is a date bar (which should be a histogram)")
            elif bar_type == 'int':
                log.debug("Bar " + bar_name + " for " + row_name + " is type int")
                # log.info("Bar " + bar_name + " for " + row_name + " is type int")
                fig, ax = plt.subplots(1, 1, figsize=(8, 8))
                # fig, ax = plt.subplots(1, 1, 'all')
                names = list(data)
                vals = data.values()

                # width = 0.7

                log.debugv("names: " + str(names))
                log.debugv("vals: " + str(vals))

                N = np.arange(len(data))

                # rects = ax.bar(N, vals, align='center', hatch=hatchs[bar_num])
                rects = ax.bar(N, vals, align='center')

                # Add value to the bars
                for rect, label in zip(rects, vals):
                    height = rect.get_height()
                    label = '{:,}'.format(label)
                    ax.text(rect.get_x() + rect.get_width() / 1.5, 1.05*height + 5, label,
                            ha='center', va='bottom', rotation=90, fontsize=7)

                # Adding extra y ticks
                yticks = list(plt.yticks()[0])
                steps = yticks[-1] - yticks[-2]
                last_tick = yticks[-1]
                extraticks = list(np.arange(yticks[-1], last_tick+2*steps, steps))
                plt.yticks(yticks + extraticks)

                # Add x ticks labels
                ax.set_xticks(N)
                ax.set_xticklabels(names)

                # Rotate x ticks labels
                for tick in ax.get_xticklabels():
                    tick.set_rotation(90)

                # Ajust the bottom of the subplot, to add space
                fig.subplots_adjust(bottom=0.35)

                # Thousands comma separator
                @ticker.FuncFormatter
                def thousand(x, pos):
                    return format(int(x), ',')
                ax.yaxis.set_major_formatter(thousand)

                # Change ticks' label size
                plt.tick_params(axis='both', which='major', labelsize=7)
                ax.set_title(bar_name + " - " + row_name)
                plt.xlabel('API names')
                plt.ylabel(bar_ylabel)

                # bottom, top = plt.ylim()
                # plt.ylim(bottom, top + 20)

                # plt.margins(0.2)
                # PLT.SUBPLOTS_ADJUST(BOTTOM=1.15)
                # plt.tight_layout(h_pad=1)
            else:
                # TODO:
                log.debug("Else")

            filename = bar_name.replace(" ", "_") + "_" + row_name
            # plt.savefig(output_dir + "/bar_"+ filename + ".png")
            # log.info("Bar saved as bar_"+ filename + ".png")
            plt.savefig(output_dir + "/bar_" + filename + ".pdf")
            log.info("Bar saved as bar_" + filename + ".pdf")

            plt.figure().clear()
            plt.close(fig)

        # Draw joint bar if there is more than one row (dataset)
        if len(datasets) > 1:

            labels = []

            # Get the labels in a list
            for row_name in joint_bar['data']:
                log.debug("Getting " + row_name)
                row_data = joint_bar['data'][row_name]

                # log.debug("Initially, " + row_name + " has a size of: " + str(len(data)))

                # Store those not previously stored
                for num, label in enumerate(row_data):
                    if label not in labels:
                        labels.insert(num, label)

            mix_data = []
            zero_array = [0 for _ in range(len(labels))]
            row_name_list = []

            for _ in range(len(datasets)):
                # Copy the array, instead of reference it
                mix_data.append(zero_array.copy())

            # Add values to the lists
            for row_num, row_name in enumerate(joint_bar['data']):
                row_data = joint_bar['data'][row_name]
                row_name_list.append(row_name)
                # for label in row_data:
                for label in labels:
                    label_index = labels.index(label)
                    if label in row_data:
                        mix_data[row_num][label_index] = row_data[label]
                    else:
                        mix_data[row_num][label_index] = orig_datasets[row_name].bar_collection[bar_name].data[label]
                log.debug("mix_data for " + str(row_num) + " has " + str(len(mix_data[row_num])) + " elements")
                log.debug(str(mix_data[row_num]))

            # fill the zeros

            for num, this_list in enumerate(mix_data):
                log.debug("list " + str(num) + " contains: " + str(this_list))

            log.debug("mix_data have length of " + str(len(mix_data)))
            # log.debug("dataset has " + str(len(data)) + " entries")

            s = "+"
            joint = s.join(joint_bar['data'].keys()).replace(" ", "_")

            if bar_type == 'date':
                log.info("Bars with dates in it")
            elif bar_type == 'int':
                log.debug("Bar " + bar_name + " is type int")
                # fig, ax = plt.subplots(1,1)

                fig, ax = plt.subplots(1, 1, figsize=(8, 8))

                width = 0.5
                widths = [0-width/2, width/2]

                # N = np.arange(len(labels))
                N = np.linspace(1, len(labels) + 2*width, len(labels))

                # for num, row_name in enumerate(mix_data):
                #     ax.bar(N+widths[num], mix_data[row_name], align='center')
                for num in range(len(mix_data)):
                    # rects = ax.bar(N+widths[num], mix_data[num], width, align='center', label=row_name_list[num],
                    #                hatch=hatchs[num])
                    rects = ax.bar(N+widths[num], mix_data[num], width, align='center', label=row_name_list[num])
                    for rect, label in zip(rects, mix_data[num]):
                        height = rect.get_height()
                        new_label = '{:,}'.format(label)
                        ax.text(rect.get_x() + rect.get_width() / 1.5, height + 5, new_label,
                                ha='center', va='bottom', rotation=90, fontsize=4)

                # Adding extra y ticks
                yticks = list(plt.yticks()[0])
                steps = yticks[-1] - yticks[-2]
                last_tick = yticks[-1]
                multiplier = 1
                extraticks = list(np.arange(yticks[-1], last_tick+multiplier*steps, steps))
                plt.yticks(yticks + extraticks)

                # Add x ticks labels
                ax.set_xticks(N)
                ax.set_xticklabels(labels)

                # Rotate x ticks labels
                for tick in ax.get_xticklabels():
                    tick.set_rotation(90)

                # Ajust the bottom of the subplot, to add space
                fig.subplots_adjust(bottom=0.45)

                plt.tick_params(axis='both', which='major', labelsize=7)
                ax.set_title(bar_name + " - " + row_name)
                plt.xlabel("API Names")
                plt.ylabel(bar_ylabel)
                # plt.xlabel(bar_xtitle)
                # plt.ylabel(bar_ytitle)
                ax.legend(loc='upper right')
            else:
                fig, ax = plt.subplots(1, 1)
                ax.bar(data, bins='auto', ec='black')
                log.debug("This title is " + bar_name + " - " + joint)
                ax.set_title(bar_name + " - " + joint)

            filename = bar_name.replace(" ", "_") + "_" + joint
            plt.savefig(output_dir + "/joint_bar_" + filename + ".pdf")
            log.info("Bar saved as joint_bar_" + filename + ".pdf")
            plt.savefig(output_dir + "/joint_bar_" + filename + ".png")
            log.info("Bar saved as joint_bar_" + filename + ".png")

            plt.figure().clear()
            plt.close(fig)

            # ==============================================================================================

            if len(datasets) == 2:
                log.debug("================================================================================")
                log.debug("Starting the diff bars for " + bar_name)
                colors = plt.rcParams['axes.prop_cycle'].by_key()['color']

                datasets = orig_datasets
                data_orig = {}

                # TODO: extend to any number of bar diagrams
                for row_name2 in datasets:
                    # for bar_name2 in datasets[row_name2].bar_collection:
                    data_orig[row_name2] = datasets[row_name2].bar_collection[bar_name].data
                    log.debug("bar for " + row_name2 + " has " + str(len(data_orig[row_name2])) + " elements")

                # log.debug("Number of labels: " + str(len(labels)))
                # log.debug("data_orig len: " + str(len(data_orig)))

                row_name_list = list(data_orig.keys())
                log.debug("row name list: " + str(row_name_list))

                # TODO: Put labels in config file
                if bar_name == "Total API calls per package":
                    main_title = "Difference in number of packages "
                    main_title2 = "Packages found only in "
                elif bar_name == "Total API packages per sample":
                    main_title = "Difference in number of packages "
                    main_title2 = "Packages found only in "
                elif bar_name == "Total API calls per method":
                    main_title = "Difference in number of methods "
                    main_title2 = "Methods found only in "
                elif bar_name == "Total API methods per sample":
                    main_title = "Difference in number of methods "
                    main_title2 = "Methods found only in "

                diff_dict = {
                    row_name_list[0]: {
                        'xlabel': "API names",
                        'ylabel': "Number of invoked methods",
                        'title': main_title + row_name_list[0] + " - " + row_name_list[1],
                        'data': {}
                    },
                    row_name_list[1]: {
                        'xlabel': "API names",
                        'ylabel': "Number of invoked methods",
                        'title': main_title + row_name_list[1] + " - " + row_name_list[0],
                        'data': {}
                    },
                    'only_mal': {
                        'xlabel': "API names",
                        'ylabel': "Number of invoked methods",
                        'title': main_title2 + row_name_list[1],
                        'data': {}
                    }
                }

                # labels = data_orig[]
                labels = set()

                # Get the labels in a list
                for row_name in data_orig:
                    log.debug("Getting " + row_name + " labels")
                    row_data = data_orig[row_name]

                    log.debug("Initially, " + row_name + " has a size of: " + str(len(row_data)))

                    for label in row_data.keys():
                        labels.add(label)

                labels = list(labels)

                my_only_mal = ['java.io.File.exists', 'java.io.FileOutputStream.<init>', 'java.io.InputStream.read']

                for num, label in enumerate(labels):

                    val = []

                    # GOOD, then MAL
                    for row_name3 in row_name_list:
                        if label in my_only_mal:
                            log.debug("Checking label " + label + " with value " + str(data_orig[row_name3][label]) + " @ " + row_name3)
                        log.debugv("Checking label " + label + " @ " + str(num))
                        if label in data_orig[row_name3].keys():
                            # TODO: Generalize for the other case
                            if label not in data_orig[row_name_list[0]].keys() and row_name3 == row_name_list[1]:
                                diff_dict['only_mal']['data'][label] = data_orig[row_name3][label]
                                if label in my_only_mal:
                                    log.debug(str(label) + " with value " + str(data_orig[row_name3][label]) + " added in only_mal")

                            val.append(data_orig[row_name3][label])
                        else:
                            val.append(0)

                    val_diff = val[0] - val[1]
                    if val_diff > 0:
                        # diff_good[label] = val_diff
                        diff_dict[row_name_list[0]]['data'][label] = val_diff
                    elif val_diff < 0:
                        # diff_mal[label] = abs(val_diff)
                        diff_dict[row_name_list[1]]['data'][label] = abs(val_diff)

                # from_mal_minus_good = diff_dict[row_name_list[1]]['data'].copy()
                # log.debug("mal diff has " + str(len(diff_dict[row_name_list[1]]['data'])) + " elements")
                from_mal_minus_good = copy.deepcopy(diff_dict[row_name_list[1]]['data'])
                log.debug("The copy has " + str(len(from_mal_minus_good)) + " elements")

                for num, name in enumerate(diff_dict):
                    diff = diff_dict[name]
                    # top_num = 30
                    top_num = 10
                    diff_data = diff['data']
                    # if len(diff_data)
                    counter = Counter(diff_data)
                    top = counter.most_common(top_num)
                    log.debug("Top of " + name + " has " + str(len(top)) + " elements")
                    # print(top)
                    diff_data = collections.OrderedDict(top).copy()
                    log.debug("printing top of " + name + ": " + str(diff_data))

                    if name == 'only_mal' and len(diff_data) < top_num:
                        log.debug("only_mal doesn't have 30 bars")
                        num_diff = top_num - len(diff_data)
                        log.debug("il le manque " + str(num_diff) + " elements")
                        num_labels_put = 0
                        for label in from_mal_minus_good:
                            if num_diff <= num_labels_put:
                                break
                            if label in diff_data:
                                log.debug("Label " + label +" exists, moving")
                                continue
                            log.debug("Adding label " + label)
                            diff_data[label] = from_mal_minus_good[label]
                            num_labels_put += 1
                        log.debug("only_mal now have " + str(len(diff_data)))

                    names = list(diff_data.keys())
                    vals = diff_data.values()

                    log.debugv("names: " + str(names))
                    log.debugv("vals: " + str(vals))
                    # width = 0.7

                    fig, ax = plt.subplots(1, 1, figsize=(8, 8))

                    N = np.arange(len(diff_data))

                    # rects = ax.bar(N, vals, align='center', color=colors[num], hatch=hatchs[num])
                    rects = ax.bar(N, vals, align='center', color=colors[num])

                    # Add value to the bars
                    for rect, label in zip(rects, vals):
                        height = rect.get_height()
                        label = '{:,}'.format(label)
                        ax.text(rect.get_x() + rect.get_width() / 1.5, 1.05*height, label,
                                ha='center', va='bottom', rotation=90, fontsize=7)

                    # Adding extra y ticks
                    yticks = list(plt.yticks()[0])
                    steps = yticks[-1] - yticks[-2]
                    last_tick = yticks[-1]
                    extraticks = list(np.arange(yticks[-1], last_tick+2*steps, steps))
                    plt.yticks(yticks + extraticks)

                    # Add x ticks labels
                    ax.set_xticks(N)
                    ax.set_xticklabels(names)

                    # Rotate x ticks labels
                    for tick in ax.get_xticklabels():
                        tick.set_rotation(90)
                        # tick.label.set_fontsize(8)

                    # Change x ticks size
                    ax.tick_params(axis='x', labelsize=8)
                    # Ajust the bottom of the subplot, to add space
                    fig.subplots_adjust(bottom=0.65)

                    # plt.xlabel(diff['xlabel'])
                    # plt.ylabel(diff['ylabel'])
                    plt.xlabel(bar_xlabel)
                    plt.ylabel(bar_ylabel)
                    plt.title(diff['title'])

                    # filename = bar_name + "_" + row_name
                    filename = "bar_" + bar_name.replace(" ", "_") + "_diff_" + name.replace(" ", "_")
                    # plt.savefig(output_dir + "/bar_"+ filename + ".png")
                    # log.info("Bar saved as bar_"+ filename + ".png")
                    plt.savefig(output_dir + "/" + filename + ".pdf")
                    log.info("Bar saved as " + filename + ".pdf")
                    plt.savefig(output_dir + "/" + filename + ".png")
                    log.info("Bar saved as " + filename + ".png")

                    plt.figure().clear()
                    plt.close(fig)

                # ====================================================================================================

                # Check if this has to do with calculation API methods

                # Top 30 most used API methods (or packages) by malware in relation to goodware
                top_total_sorted = {
                    row_name_list[0]: {},
                    row_name_list[1]: {}
                }

                # top_num = 30
                top_num = 10
                diff_data = diff_dict[row_name_list[1]]['data']
                counter = Counter(diff_data)
                top = counter.most_common(top_num)
                log.debug("Top of " + name + " has " + str(len(top)) + " elements")
                # print(top)
                diff_data = collections.OrderedDict(top)
                # top_diff_mal_vs_good = {}
                log.debug("Printing labels in diff_dict " + row_name_list[1] + ": "
                          + str(diff_data))
                # text_height = 0
                for label in diff_data:
                # for label in top_data[row_name_list[1]]:
                    log.debug("Adding " + label + " to top 30")
                    for row_name2 in row_name_list:
                        if label in data_orig[row_name2]:
                            top_total_sorted[row_name2][label] = data_orig[row_name2][label]
                        else:
                            top_total_sorted[row_name2][label] = 0
                        # if text_height < top_total_sorted[row_name2][label]:
                            # text_height = top_total_sorted[row_name2][label]
                    # print(label)

                # text_height = 1.05*text_height + 5

                names = list(diff_data.keys())

                for n, myname in enumerate(names):
                    log.debug("My old name was: " + myname)
                    names[n] = '.'.join(myname.split('.')[-2:])
                    log.debug("My new name is: " + myname)

                # vals = top_total_sorted.values()

                log.debugv("names: " + str(names))
                log.debugv("vals: " + str(vals))
                # width = 0.7

                fig, ax = plt.subplots(1, 1, figsize=(6, 6))

                # N = np.arange(len(top_total_sorted))
                width = 0.5
                widths = [0-width/2, width/2]

                # N = np.arange(len(labels))
                # N = np.linspace(1, len(diff_data) + 2*width, len(diff_data))
                N = np.linspace(1, len(diff_data) + width, len(diff_data))

                # rects = ax.bar(N, vals, align='center', color=colors[num], hatch=hatchs[num])
                # for num in range(len(top_total_sorted)):
                for num, row_name80 in enumerate(top_total_sorted):
                    # names = top_total_sorted[row_name80].keys()
                    vals = top_total_sorted[row_name80].values()
                    # rects = ax.bar(N+widths[num], mix_data[num], width, align='center', label=row_name_list[num],
                    #                hatch=hatchs[num])
                    rects = ax.bar(N+widths[num], vals, width, align='center',
                                   label=row_name80)
                    for rect, label in zip(rects, top_total_sorted[row_name80]):
                        height = rect.get_height()
                        new_label = '{:,}'.format(top_total_sorted[row_name80][label])
                        ax.text(rect.get_x() + rect.get_width() / 1.5, 1.02*height,
                                new_label, ha='center', va='bottom',
                                rotation=90, fontsize=8)

                # Add value to the bars
                # for rect, label in zip(rects, vals):
                #     height = rect.get_height()
                #     label = '{:,}'.format(label)
                #     ax.text(rect.get_x() + rect.get_width() / 1.5, height + 5, label,
                #             ha='center', va='bottom', rotation=90, fontsize=7)

                # Adding extra y ticks
                yticks = list(plt.yticks()[0])
                steps = yticks[-1] - yticks[-2]
                last_tick = yticks[-1]
                extraticks = list(np.arange(yticks[-1], last_tick+2*steps, steps))
                plt.yticks(yticks + extraticks)

                # Add x ticks labels
                ax.set_xticks(N)
                ax.set_xticklabels(names)

                # Rotate x ticks labels
                for tick in ax.get_xticklabels():
                    tick.set_rotation(90)
                    # tick.set_rotation(45)
                # for tick in ax.xaxis.get_major_ticks():
                #     tick.label.set_fontsize(2)

                # Change x ticks size
                ax.tick_params(axis='x', labelsize=10)
                # Ajust the bottom of the subplot, to add space
                fig.subplots_adjust(bottom=0.55)

                # plt.xlabel(diff['xlabel'])
                # plt.ylabel(diff['ylabel'])
                plt.xlabel(bar_xlabel)
                plt.ylabel(bar_ylabel)
                # plt.title("Top " + str(top_num) + " most used API methods\nby malware in relation to goodware")
                ax.legend(loc='upper center')
                # filename = bar_name + "_" + row_name
                filename = "bar_top_30_most_used_malware_vs_goodware"
                # plt.savefig(output_dir + "/bar_"+ filename + ".png")
                # log.info("Bar saved as bar_"+ filename + ".png")
                plt.savefig(output_dir + "/" + filename + ".pdf")
                log.info("Bar saved as " + filename + ".pdf")
                plt.savefig(output_dir + "/" + filename + ".png")
                log.info("Bar saved as " + filename + ".png")

                plt.figure().clear()
                plt.close(fig)


def output_to_files(datasets, out_dir):
    log.info("Processing finished, outputing files")

    jsonfile = out_dir + "/" + OUTPUT_FILENAME + ".json"
    xlsxfile = out_dir + "/" + OUTPUT_FILENAME + ".xlsx"
    rawfile = out_dir + "/" + OUTPUT_FILENAME + "_raw.json"
    latexfile = out_dir + "/" + OUTPUT_FILENAME + ".tex"

    # Dictionary for the output JSON file
    dico = {}
    # dico = datasets
    # Raw results of the histogram
    raw = {}
    row_num = 1

    # Initialize a workbook
    wb = Workbook()

    # Grab the active worksheet
    ws = wb.active

    # Create names for columns in workbook
    for row_name in datasets:
        row = datasets[row_name]
        ws.cell(row=row_num, column=2, value="Total")
        col_num = 3
        for column in row.columns:
            ws.cell(row=row_num, column=col_num, value=column.name)
            ws.merge_cells(start_row=row_num, start_column=col_num, end_row=row_num, end_column=col_num+1)

            col_num += 2

    row_num += 1
    col_num = 1
    dataset_dump = {}

    # If the result JSON already exists, don't do anything
    # if not os.path.isfile(jsonfile):
    for row_name in datasets:
        log.debug("Assigning row " + row_name)

        dico[row_name] = {}
        raw[row_name] = {}

        row = datasets[row_name]

        # Assigning the name of the row
        ws.cell(row=row_num, column=col_num, value=row_name)
        col_num += 1

        # Assigning the total of the row
        ws.cell(row=row_num, column=col_num, value=row.total)
        ws.merge_cells(start_row=row_num, start_column=col_num, end_row=row_num+1, end_column=col_num)
        col_num += 1

        if len(row.columns) != 0:
            for column in row.columns:
                log.debug("Assiging column " + column.name)
                column.get_total()
                if column.req_type is None:
                    dico[row_name][column.name] = [column.total, column.pct_total, column.pct_depend]
                    ws.cell(row=row_num, column=col_num, value=column.total)
                    ws.cell(row=row_num+1, column=col_num, value=column.pct_total)
                    ws.cell(row=row_num+1, column=col_num+1, value=column.pct_depend)
                    # Grand total merge_cells
                    ws.merge_cells(start_row=row_num, start_column=col_num, end_row=row_num, end_column=col_num+1)
                else:
                    dico[row_name][column.name] = column.res_poll
                    ws.cell(row=row_num, column=col_num, value=str(column.res_poll)
                            .replace("{", "").replace("}", "").replace(",", "\n"))
                    ws.merge_cells(start_row=row_num, start_column=col_num, end_row=row_num+1, end_column=col_num+1)
                col_num += 2
            # Dataset name merge
            ws.merge_cells(start_row=row_num, start_column=1, end_row=row_num+1, end_column=1)
            row_num += 2
            col_num = 1
        # print(str(dico))
        if 'histograms' in row.histogram_collection.keys():
            for histogram_name in row.histogram_collection:
                # def output_histograms(data,output_dir):
                data = row.histogram_collection[histogram_name]['data']
                raw[row_name][histogram_name] = row.histogram_collection[histogram_name]

        # TODO: Add histograms to dump
        dataset_dump[row_name] = {'bars': {}}
        row_dump = dataset_dump[row_name]

        if len(row.bar_collection) != 0:
            for bar_name in row.bar_collection:
                bar_dump = {}

                bar = row.bar_collection[bar_name]
                bar_dump['data'] = bar.data
                bar_dump['req'] = bar.request
                bar_dump['type'] = bar.type

                row_dump['bars'][bar_name] = bar_dump
    # Create the output directory if it doesn't exists
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)

    with open(jsonfile, 'w') as out:
        json.dump(dico, out)
    log.info("File saved at " + jsonfile)

    # with open(rawfile, 'w') as out:
    #     json.dump(raw, out)
    # log.info("File saved at " + rawfile)

    # for row_name in datasets:
    #     print("my name is " + row_name)

    with open(rawfile, 'w') as out:
        json.dump(dataset_dump, out)
    log.info("File saved at " + rawfile)
    # with open(rawfile, 'w') as out:
    #     json.dump(datasets, out)
    # log.info("File saved at " + rawfile)

    # filename = "res.xlsx"
    wb.save(xlsxfile)
    log.info("File saved at " + xlsxfile)
    # else:
    #     log.warning("file already written. finishing")


    # Output latex file
    with open(latexfile, 'w') as out:
        if len(datasets) > 0:
            first_row = list(datasets.values())[0]

            nb_column = len(first_row.columns)
            out.write('\\begin{tabular}{|%s|}\n' % ('|'.join('c'*(nb_column+2))))

            columns_names = [column.name for column in first_row.columns]
            columns_names = ["", "Total"] + columns_names
            out.write(" & ".join(columns_names) + " \\\\ \\hline\n")
            
            for (row_name, row) in datasets.items():
                line1 = []
                line2 = []
                line1.append('\\multirow{2}{*}{%s}' % row_name)
                line1.append('\\multirow{2}{*}{%s}' % str(row.total))
                line2.append('')
                line2.append('')
                for column in row.columns:
                    line1.append(str(column.total))
                    line2.append(str(column.pct_depend)+"\\%")
                out.write(" & ".join(line1) + " \\\\\n")
                out.write(" & ".join(line2) + " \\\\ \\hline\n")
            out.write('\\end{tabular}\n')

    log.info("File saved at " + latexfile)
        


def topN(dico, N, poll_type):
    if poll_type == 'top':
        res = dict(Counter(dico).most_common(N))
    elif poll_type == 'bottom' or poll_type == 'bot':
        res = dict(Counter(dico).most_common()[:-N-1:-1])
    return res

################################################################################
#                                                                              #
#                          Row and column definition                           #
#                                                                              #
################################################################################


class Bar:
    def __init__(self, req, type):
        self.request = req
        # self.req_parser = None
        self.poll = {}
        self.res_poll = {}
        # self.data = []
        self.data = {}
        self.type = type
        if len(self.request.split(":")) < 2:
            # Normal request
            self.req_type = None
            self.jpath = self.request.split(" ")[0]
            # self.req_parser = BooleanParser.BooleanParser(self.req)
        else:
            # This is poll request, it only needs to retrive a value
            # self.req_type can be top or bottom
            self.jpath, self.req_type, self.num_of_poll = self.request.split(":")
            self.num_of_poll = int(self.num_of_poll)


class Histogram:
    def __init__(self, req, type):
        self.request = req
        # self.req_parser = None
        self.poll = {}
        self.res_poll = {}
        self.data = []
        self.type = type
        if len(self.req.split(":")) < 2:
            # Normal request
            self.req_type = None
            self.jpath = self.req.split(" ")[0]
            # self.req_parser = BooleanParser.BooleanParser(self.req)
        else:
            # This is poll request, it only needs to retrive a value
            # self.req_type can be top or bottom
            self.jpath, self.req_type, self.num_of_poll = self.req.split(":")
            self.num_of_poll = int(self.num_of_poll)


class Column:
    def __init__(self, name, parent_row, req, depend=None):
        self.name = name
        self.total = self.pct_total = self.pct_depend = 0
        self.parent_row = parent_row
        self.req = req
        # self.req_parser = None
        self.poll = {}
        self.res_poll = {}
        # print("The req is " + str(self.req))
        # print("The length is " + str(len(self.req.split(":"))))
        if len(self.req.split(":")) < 2:
            # Normal request
            self.req_type = None
            self.jpath = self.req.split(" ")[0]
            self.req_parser = BooleanParser.BooleanParser(self.req)
        else:
            # This is poll request, it only needs to retrive a value
            # self.req_type can be top or bottom
            self.jpath, self.req_type, self.num_of_poll = self.req.split(":")
            self.num_of_poll = int(self.num_of_poll)
        self.depend = depend
        # print("Column " + self.name  + " created")

    def get_total(self):
        if self.req_type is None:
            # If there is no samples in the column, throw a warning
            if self.total == 0:
                log.warning(self.name + ": The total is 0, cannot divide by 0")
            else:
                # Calculate the percentage by total
                self.pct_total = (self.total / self.parent_row.total)*100
                # If no dependence is declared, the percentage is by total
                if self.depend is None:
                    self.pct_depend = self.pct_total
                else:
                    self.pct_depend = (self.total / self.depend.total)*100
        else:
            # print('getting poll')
            self.res_poll = topN(self.poll, self.num_of_poll, self.req_type)


class Row:
    var_dict = {}

    def __init__(self, name):
        self.name = name
        self.total = 0
        # self.
        self.columns = []
        self.histogram_collection = {}
        self.bar_collection = {}
        # print("Row " + self.name + " created")

    def create_column(self, name, req, depend_name=None):
        depend = None
        # If the name of the column for getting the percentage is different from None
        if depend_name is not None:
            # Check if the name exists:
            # Find the name in self.columns "column_name"
            # If the tuple is empty, the name was not found, depends is None
            column_name = (column for column in self.columns if column.name == depend_name)
            depend = next(column_name, None)
        # print('this depend is ' + str(depend))
        new_column = Column(name, self, req, depend)
        self.columns.append(new_column)

    def create_histogram(self, name, request, type):
        # self.histogram_collection[name] = {"type": type, "request": request, "data": []}
        new_histogram = Histogram(request, type)
        self.histogram_collection[name] = new_histogram

    def create_bar(self, name, request, type):
        # self.bar_collection[name] = {"type": type, "request": request, "data": []}
        new_bar = Bar(request, type)
        self.bar_collection[name] = new_bar

    def process(self, json_file):
        self.total += 1

        if len(self.columns) != 0:
            for column in self.columns:
                log.debugv('Processing data ' + str(self.total))
                log.debugv('Parsing: ' + str(column.name))

                # Find the value of the JSONPath expression if it's not in the dictionary
                expression = column.jpath

                if expression not in self.var_dict.keys():
                    try:
                        parse = expression.split('.')
                        name = json_file['name']
                        log.debugv('the parsed list is: ' + str(parse))
                        log.debugv('My name is: ' + str(parse[2]))
                        self.var_dict[expression] = json_file[name][parse[2]][parse[3]]  # This may fail:
                        # The dict doesn't have certain key
                        if self.var_dict[expression] == "":
                            self.var_dict[expression] = 0
                    except Exception as e:
                        log.debugv("This happened during the parsing: " + str(e))
                        self.var_dict[expression] = None
                    # self.var_dict[expression] = parse(expression).find(json_file)
                log.debugv("This is the result of the parsing: " + str(self.var_dict[expression]))
                # If the expression's value is None (which it was not found in the JSON),
                # do nothing. Else, add the value
                if self.var_dict[expression] is not None:
                    # val = self.var_dict[expression][0].value
                    val = self.var_dict[expression]
                    log.debugv("The value for " + str(expression) + " is: " + str(val))
                    # Put some quotes to strings, so Jason can be happy :DDDD
                    # Check if string is not int or float
                    if not isinstance(val, int) and not isinstance(val, float):
                        log.debugv('There is an instance of string for val: ' + str(val))
                        val = "\"" + val + "\""
                    elif isinstance(val, bool):
                        log.debugv('There is an instance of bool for val: ' + str(val))
                        val = "\"" + str(val) + "\""
                    if column.req_type is None:
                        log.debugv('changing ' + column.jpath + ' -> ' + str(val))
                        evaluator = column.req_parser
                        log.debugv('The request to evaluate is: ' + str(evaluator.tokenizer.expression))
                        # {expression:val} => change 'expression' to 'val' when evaluating
                        res = evaluator.evaluate({expression: val})
                        log.debugv("The result was: " + str(res))
                        if res:
                            column.total += 1
                    else:
                        val = str(val)
                        if val in column.poll.keys():
                            column.poll[val] += 1
                        else:
                            column.poll[val] = 1

        # Creating histograms ==========================================================================================
        if (self.histogram_collection.keys()) != 0:
            for histogram_name in self.histogram_collection:
                # histogram_dict = self.histogram_collection[histogram_name]
                histogram = self.histogram_collection[histogram_name]

                # Find the value of the JSONPath expression if it's not in the dictionary
                # expression = histogram_dict['request']
                expression = histogram.request

                # Check if is value, dict, or list

                if expression not in self.var_dict.keys():
                    try:
                        name = json_file['name']
                        parse = expression.split('.')
                        for i, expr in enumerate(parse):
                            if i == 1:
                                expression_val = json_file[name]
                            else:
                                expression_val = expression[expr]
                        log.debugv('the parsed list is: ' + str(parse))
                        # log.debugv('My name is: ' + str(parse[2]))
                        expression_class = expression_val.__class__
                        if expression_class == int or expression_class == float or expression_class == str \
                                or expression_class == bool:
                            # This may fail
                            # self.var_dict[expression] = json_file[name][parse[2]][parse[3]]
                            self.var_dict[expression] = expression_val
                            if histogram.type == 'date' and self.var_dict[expression] == "":
                                log.debugv('This date "' + expression + '" is now 0')
                                self.var_dict[expression] = 0
                        elif expression_class == dict:
                            print(expression + " is a dictionnary")

                        elif expression_class == list:
                            print(expression + " is a list")

                    except Exception as e:
                        if histogram.type == 'data':
                            self.var_dict[expression] = 0
                        else:
                            self.var_dict[expression] = None
                log.debugv("--- Histogram: This is the result of the parsing: " + str(self.var_dict[expression]))

                # Add the value if it exists
                if self.var_dict[expression] is not None:
                    val = self.var_dict[expression]
                    log.debugv("The value for " + str(expression) + " is: " + str(val))
                    # if histogram_dict['type'] == 'int':
                    if histogram.req_type == 'int':
                        # histogram_dict['data'].append(int(val))
                        histogram.data.append(int(val))
                    else:
                        # histogram_dict['data'].append(val)
                        histogram.data.append(val)
        else:
            log.debugv("No histograms, moving on")

        # Creating bars ================================================================================================
        if (self.bar_collection.keys()) != 0:
            # print("hello")
            for bar_name in self.bar_collection:
                # bar_dict = self.bar_collection[bar_name]
                bar = self.bar_collection[bar_name]

                # Find the value of the JSONPath expression if it's not in the dictionary
                # expression = bar_dict['request']
                expression = bar.request

                # Check if is value, dict, or list
                log.debugv("Bar request: " + str(expression))

                if expression not in self.var_dict.keys():
                    # try:
                    name = json_file['name']
                    parse = expression.split('.')
                    request_tail = ""
                    log.debugv('the parsed list is: ' + str(parse))
                    for i, expr in enumerate(parse):
                        log.debugv("parse number: " + str(i) + ", expr is: " + str(expr))
                        if i == 1:
                            expression_val = json_file[name]
                        elif i > 1:
                            if expr == '*':
                                if i+1 == len(parse)-1:
                                    # For the use of # 
                                    request_tail = parse[i+1]
                                else:
                                    request_tail = expr
                                break
                            elif expr == '#':
                                request_tail = expr
                            else:
                                expression_val = expression_val[expr]
                    # log.debugv('My name is: ' + str(parse[2]))
                    log.debugv("expression_val: " + str(expression_val))
                    expression_class = expression_val.__class__
                    log.debugv('expression is of ' + str(expression_class))
                    if expression_class == int or expression_class == float or expression_class == str \
                            or expression_class == bool:
                        # This may fail
                        # self.var_dict[expression] = json_file[name][parse[2]][parse[3]]
                        self.var_dict[expression] = expression_val
                        if bar.type == 'date' and self.var_dict[expression] == "":
                            log.debugv('This date "' + expression + '" is now 0')
                            self.var_dict[expression] = 0
                    # As the expression gets a dict, add the key/values for all according to the last key in the request
                    elif expression_class == dict:
                        log.debugv(expression + " is a dictionnary")
                        log.debugv("The request tail is " + str(request_tail))
                        # Add keys and values directly to dictionnary
                        # log.debugv("This dictonnary contains: " + str(expression_val))
                        # For each key, add the value
                        for key in expression_val:
                            # Skip status
                            if key == "status":
                                continue
                            if request_tail == "#":
                                # If the request has a # at the end, it will count if the package appear
                                # in the searched
                                # this_dict = expression_val[key]
                                # log.debugv("this_dict contains: " + str(this_dict))
                                # value = 0
                                # log.debugv(name + ": Adding value of " + key)
                                # for key in this_dict:
                                    # value += this_dict[key1]
                                if key is "status":
                                    continue
                                # key.split will devide the string by the character put in parenthesis
                                # then, join with 'glue' the list with the string that calls the join function
                                package = '.'.join(key.split('.')[:-1])
                                if package not in self.var_dict:
                                    self.var_dict[package] = 1
                                # else:
                                #     self.var_dict[key] = 1
                            elif request_tail == '*':
                                this_dict = expression_val[key]
                                for key1 in this_dict:
                                    # value = this_dict[key1]
                                    compound_key_name = key + "." + key1
                                    if compound_key_name in self.var_dict:
                                        # self.var_dict[compound_key_name] += value
                                        self.var_dict[compound_key_name] += 1
                                    else:
                                        # self.var_dict[compound_key_name] = value
                                        self.var_dict[compound_key_name] = 1
                            else:
                                value = expression_val[key][request_tail]
                                log.debugv("The key is:" + str(key) + "and the value is: " + str(value))

                            # if request_tail != '*':
                            #     if key in self.var_dict:
                            #         self.var_dict[key] += value
                            #     else:
                            #         self.var_dict[key] = value
                    elif expression_class == list:
                        log.debugv(expression + " is a list")

                    # time.sleep(30)

                    # except Exception as e:
                    #     log.warning("An excepcion occured: " + str(e))
                    #     if bar.type == 'data':
                    #         self.var_dict[expression] = 0
                    #     else:
                    #         self.var_dict[expression] = None
                # log.debugv("--- Bar: This is the result of the parsing: " + str(self.var_dict))

                for label in self.var_dict:
                    if label in bar.data:
                        bar.data[label] += self.var_dict[label]
                    else:
                        bar.data[label] = self.var_dict[label]

            # bar.data = Counter(bar.data) + Counter(self.var_dict)

            # log.info("bar data is of " + str(bar.data.__class__))
                # Add the value if it exists
                # if self.var_dict[expression] is not None:
                #     val = self.var_dict[expression]
                #     log.debugv("The value for " + str(expression) + " is: " + str(val))
                #     # if bar_dict['type'] == 'int':
                #     if bar.req_type == 'int':
                #         # bar_dict['data'].append(int(val))
                #         bar.data.append(int(val))
                #     else:
                #         # bar_dict['data'].append(val)
                #         bar.data.append(val)
        else:
            log.debugv("No bars, moving on")

        # time.sleep(1)

        # Reinitialize the dictionary
        self.var_dict = {}

################################################################################
#                                                                              #
#                  postprocessing function (primary function)                  #
#                                                                              #
################################################################################


def postprocessing(myjsonconfig, verbose=0):

    # print('This verbosity is ' + str(verbose))
    # quit()
    #
    # logSetup(verbose)

    t_start = time.time()
    filename = myjsonconfig

    # Check if the JSON config file exists
    if not os.path.isfile(filename):
        log.warning("The file " + filename + " doesn't exist. Quitting")
    else:
        with open(filename) as config:
            myjson = json.load(config)

        log.debugv("This is the config file: " + str(myjson))

        log.info("Output dir: " + myjson['output_dir'])
        # jsonfile = myjson['output_dir'] + "/" + OUTPUT_FILENAME + ".json"

        # Initialize Row and Columns objects with an empty dataset (dictionary)
        datasets = {}

        res_json = myjson['output_dir'] + "/res_raw.json"

        if os.path.isfile(res_json):
            log.debug("Loading raw JSON file")
            with open(res_json) as res_json_obj:
                datasets_raw = json.load(res_json_obj)
            log.debug("Load complete")

            for row_name in datasets_raw:
                log.debug("Creating row " + row_name)
                row_raw = datasets_raw[row_name]
                datasets[row_name] = Row(row_name)
                this_row = datasets[row_name]
                if 'bars' in row_raw.keys():
                    for bar_name in row_raw['bars']:
                        log.debug("Creating bar " + bar_name)
                        bar_raw = row_raw['bars'][bar_name]
                        bar_request = bar_raw['req']
                        bar_type = bar_raw['type']
                        this_row.create_bar(bar_name, bar_request, bar_type)
                        this_row.bar_collection[bar_name].data = bar_raw['data']
        else:
            print("no res json file (?)")

        # for each row, because they are different datasets
        log.debug("Length of datasets is " + str(len(datasets)))
        if len(datasets) == 0:
            for row in myjson['rows']:

                log.info('Processing ' + row)

                # create row object
                datasets[row] = Row(row)

                # create columns
                if 'columns' in myjson.keys():
                    for column in myjson['columns']:
                        log.debugv("the column is " + str(column))
                        # create_column(self,name,req,depends=None):
                        datasets[row].create_column(column, myjson['columns'][column][0], myjson['columns'][column][1])
                else:
                    log.debugv("There are no columns in the config file, moving on")

                # Create histograms
                if 'histograms' in myjson.keys():
                    for histogram in myjson['histograms']:
                        log.debugv("Adding " + histogram + " histogram")
                        # Each row will contain the same values: name, request (JSONPath), and type
                        histogram_request = myjson['histograms'][histogram][0]
                        histogram_type = myjson['histograms'][histogram][1]
                        datasets[row].create_histogram(histogram, histogram_request, histogram_type)
                else:
                    log.debugv("There are no histograms in the config file, moving on")

                # Create bars
                if 'bars' in myjson.keys():
                    for bar in myjson['bars']:
                        log.debugv("Adding " + bar + " bar")
                        # Each row will contain the same values: name, request (JSONPath), and type
                        bar_request = myjson['bars'][bar][0]
                        bar_type = myjson['bars'][bar][1]
                        datasets[row].create_bar(bar, bar_request, bar_type)
                else:
                    log.debugv("There are no bars in the config file, moving on")

                # TODO: If dataset dir doesn't exists, continue
                mypath = myjson['rows'][row]

                try:
                    files = [f for f in listdir(mypath) if isfile(join(mypath, f)) and f.endswith(".json")]
                except Exception:
                    raise Exception("Cannot open dir")
                    continue

                # Process each file into the rows
                numFiles = len(files)
                log.info("Processing " + str(numFiles) + " files")
                for filename in files:

                    log.debugv(row + ": Processing file number " + str(numFiles) + ": " + filename + " JSON file")
                    numFiles -= 1
                    with open(mypath + "/" + filename) as f:
                        mwjson = json.load(f)

                    # Get the name of the file (without the extension)
                    mwjson['name'] = filename.split('.')[0]
                    datasets[row].process(mwjson)

                if verbose >= 1:
                    if 'histograms' in myjson.keys():
                        for histo in myjson['histograms']:
                            log.debug("Dataset " + row + " has " + str(len(datasets[row].histogram_collection[histo].data)) + " entries in " + histo)

            output_to_files(datasets, myjson['output_dir'])

            t_end = time.time()
            log.info("PROCESS TIME: " + str(round(t_end - t_start, 1)) + " s")

        if 'histograms' in myjson.keys():
            if verbose == 2:
                for name in datasets:
                    log.debugv("This dataset has evaludated " + str(datasets[name].total) + " entries")
                    for histo in myjson['histograms']:
                        log.debugv("Dataset " + datasets[name].name + " has " + str(len(datasets[name].histogram_collection[histo]['data'])) + " entries in " + histo)

            output_histograms(datasets, myjson['histograms'], myjson['output_dir'])

        if 'bars' in myjson.keys():
            output_bars(datasets, myjson['bars'], myjson['output_dir'])

    t_end = time.time()
    log.info("TOTAL TIME: " + str(round(t_end - t_start, 1)) + " s")
    quit()


if __name__ == "__main__":
    # print("your arguments are: " + str(sys.argv))
    # Doc : https://docs.python.org/3/howto/argparse.html#id1
    parser = argparse.ArgumentParser(description="PostProcessing, for json files generated by the orchestrator.")
    parser.add_argument('json_config_file', help='The path to the JSON config file')
    parser.add_argument('-s', help='Optrion for StatsInvoke')
    parser.add_argument('-v', help='Output information to the standart output (-vv is very verbose)', action="count")

    # parser.add_argument('-H', metavar='Result JSON file', help='Output information to the standart output (-vv is very verbose)')

    args = parser.parse_args()

    if args.v is not None:
        verbosity = args.v
    else:
        verbosity = 0

    logSetup(verbosity)
    # log.info('This verbosity is ' + str(verbosity))
    # quit()
    postprocessing(args.json_config_file, verbosity)

    quit()
